------------- Training -------------
40670
8134
epochs: 196
epochs per stage: 49

Epoch 00001: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 1/196
2019-03-16 19:44:00.410854: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-03-16 19:44:01.758464: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-03-16 19:44:06.233731: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profile Session started.
2019-03-16 19:44:06.298852: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcupti.so.10.0
   1/1016 [..............................] - ETA: 5:10:06 - loss: 3.0827 - mse: 5.8480 - mace: 98.6451WARNING: Logging before flag parsing goes to stderr.
W0316 19:44:06.768089 140313707435776 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.223046). Check your callbacks.
   2/1016 [..............................] - ETA: 2:36:06 - loss: 2.   3/1016 [..............................] - ETA: 1:44:15 - loss: 2.   5/1016 [..............................] - ETA: 1:02:46 - loss: 2.   6/1016 [..............................] - ETA: 52:23 - loss: 2.63   8/1016 [..............................] - ETA: 39:26 - loss: 2.45   9/1016 [..............................] - ETA: 35:07 - loss: 2.38  11/1016 [..............................] - ETA: 28:49 - loss: 2.27  12/1016 [..............................] - ETA: 26:28 - loss: 2.21  13/1016 [..............................] - ETA: 24:29 - loss: 2.16  14/1016 [..............................] - ETA: 22:46 - loss: 2.12  15/1016 [..............................] - ETA: 21:17 - loss: 2.07  16/1016 [..............................] - ETA: 20:00 - loss: 2.03  18/1016 [..............................] - ETA: 17:50 - loss: 1.94  19/1016 [..............................] - ETA: 16:55 - loss: 1.89  20/1016 [..............................] - ETA: 16:06 - loss: 1.85  21/1016 [..............................] - ETA: 15:22 - loss: 1.81  22/1016 [..............................] - ETA: 14:42 - loss: 1.77  23/1016 [..............................] - ETA: 14:05 - loss: 1.73  24/1016 [..............................] - ETA: 13:31 - loss: 1.70  25/1016 [..............................] - ETA: 13:00 - loss: 1.67  26/1016 [..............................] - ETA: 12:32 - loss: 1.64  27/1016 [..............................] - ETA: 12:05 - loss: 1.61  28/1016 [..............................] - ETA: 11:40 - loss: 1.58  29/1016 [..............................] - ETA: 11:17 - loss: 1.55  30/1016 [..............................] - ETA: 10:56 - loss: 1.53  31/1016 [..............................] - ETA: 10:36 - loss: 1.51  32/1016 [..............................] - ETA: 10:17 - loss: 1.48  33/1016 [..............................] - ETA: 9:59 - loss: 1.467  34/1016 [>.............................] - ETA: 9:42 - loss: 1.448  35/1016 [>.............................] - ETA: 9:27 - loss: 1.429  36/1016 [>.............................] - ETA: 9:12 - loss: 1.413  37/1016 [>.............................] - ETA: 8:58 - loss: 1.397  38/1016 [>.............................] - ETA: 8:45 - loss: 1.381  39/1016 [>.............................] - ETA: 8:32 - loss: 1.366  40/1016 [>.............................] - ETA: 8:20 - loss: 1.352  41/1016 [>.............................] - ETA: 8:09 - loss: 1.338  42/1016 [>.............................] - ETA: 7:58 - loss: 1.326  43/1016 [>.............................] - ETA: 7:47 - loss: 1.312  44/1016 [>.............................] - ETA: 7:37 - loss: 1.300  45/1016 [>.............................] - ETA: 7:28 - loss: 1.290  46/1016 [>.............................] - ETA: 7:19 - loss: 1.278  47/1016 [>.............................] - ETA: 7:10 - loss: 1.267  48/1016 [>.............................] - ETA: 7:02 - loss: 1.257  49/1016 [>.............................] - ETA: 6:54 - loss: 1.247  50/1016 [>.............................] - ETA: 6:46 - loss: 1.237  51/1016 [>.............................] - ETA: 6:39 - loss: 1.228  52/1016 [>.............................] - ETA: 6:32 - loss: 1.218  53/1016 [>.............................] - ETA: 6:25 - loss: 1.209  54/1016 [>.............................] - ETA: 6:18 - loss: 1.201  55/1016 [>.............................] - ETA: 6:12 - loss: 1.193  56/1016 [>.............................] - ETA: 6:06 - loss: 1.186  57/1016 [>.............................] - ETA: 6:00 - loss: 1.178  58/1016 [>.............................] - ETA: 5:54 - loss: 1.172  59/1016 [>.............................] - ETA: 5:49 - loss: 1.165  60/1016 [>.............................] - ETA: 5:43 - loss: 1.158  61/1016 [>.............................] - ETA: 5:38 - loss: 1.152  62/1016 [>.............................] - ETA: 5:33 - loss: 1.146  63/1016 [>.............................] - ETA: 5:28 - loss: 1.139  64/1016 [>.............................] - ETA: 5:24 - loss: 1.133  65/1016 [>.............................] - ETA: 5:19 - loss: 1.128  67/1016 [>.............................] - ETA: 5:10 - loss: 1.118  68/1016 [=>............................] - ETA: 5:06 - loss: 1.113  69/1016 [=>............................] - ETA: 5:02 - loss: 1.109  70/1016 [=>............................] - ETA: 4:58 - loss: 1.104  71/1016 [=>............................] - ETA: 4:54 - loss: 1.100  72/1016 [=>............................] - ETA: 4:51 - loss: 1.096  73/1016 [=>............................] - ETA: 4:47 - loss: 1.091  74/1016 [=>............................] - ETA: 4:44 - loss: 1.087  75/1016 [=>............................] - ETA: 4:40 - loss: 1.083  76/1016 [=>............................] - ETA: 4:37 - loss: 1.078  77/1016 [=>............................] - ETA: 4:34 - loss: 1.074  78/1016 [=>............................] - ETA: 4:31 - loss: 1.070  79/1016 [=>............................] - ETA: 4:28 - loss: 1.066  80/1016 [=>............................] - ETA: 4:25 - loss: 1.062  81/1016 [=>............................] - ETA: 4:22 - loss: 1.059  82/1016 [=>............................] - ETA: 4:19 - loss: 1.055  83/1016 [=>............................] - ETA: 4:16 - loss: 1.052  84/1016 [=>............................] - ETA: 4:13 - loss: 1.049  85/1016 [=>............................] - ETA: 4:11 - loss: 1.046  86/1016 [=>............................] - ETA: 4:08 - loss: 1.043  87/1016 [=>............................] - ETA: 4:06 - loss: 1.040  88/1016 [=>............................] - ETA: 4:03 - loss: 1.037  89/1016 [=>............................] - ETA: 4:01 - loss: 1.034  90/1016 [=>............................] - ETA: 3:58 - loss: 1.032  91/1016 [=>............................] - ETA: 3:56 - loss: 1.029  92/1016 [=>............................] - ETA: 3:54 - loss: 1.026  93/1016 [=>............................] - ETA: 3:52 - loss: 1.023  94/1016 [=>............................] - ETA: 3:49 - loss: 1.020  95/1016 [=>............................] - ETA: 3:47 - loss: 1.017  96/1016 [=>............................] - ETA: 3:45 - loss: 1.014  97/1016 [=>............................] - ETA: 3:43 - loss: 1.011  98/1016 [=>............................] - ETA: 3:41 - loss: 1.008  99/1016 [=>............................] - ETA: 3:39 - loss: 1.006 100/1016 [=>............................] - ETA: 3:37 - loss: 1.004 101/1016 [=>............................] - ETA: 3:35 - loss: 1.001 102/1016 [==>...........................] - ETA: 3:33 - loss: 0.999 103/1016 [==>...........................] - ETA: 3:32 - loss: 0.997 104/1016 [==>...........................] - ETA: 3:30 - loss: 0.995 105/1016 [==>...........................] - ETA: 3:28 - loss: 0.993 106/1016 [==>...........................] - ETA: 3:26 - loss: 0.991 107/1016 [==>...........................] - ETA: 3:25 - loss: 0.989 108/1016 [==>...........................] - ETA: 3:23 - loss: 0.986 109/1016 [==>...........................] - ETA: 3:21 - loss: 0.984 110/1016 [==>...........................] - ETA: 3:20 - loss: 0.983 111/1016 [==>...........................] - ETA: 3:18 - loss: 0.981 112/1016 [==>...........................] - ETA: 3:17 - loss: 0.979 113/1016 [==>...........................] - ETA: 3:15 - loss: 0.977 114/1016 [==>...........................] - ETA: 3:14 - loss: 0.975 115/1016 [==>...........................] - ETA: 3:12 - loss: 0.973 116/1016 [==>...........................] - ETA: 3:11 - loss: 0.972 117/1016 [==>...........................] - ETA: 3:09 - loss: 0.970 118/1016 [==>...........................] - ETA: 3:08 - loss: 0.968 119/1016 [==>...........................] - ETA: 3:06 - loss: 0.967 120/1016 [==>...........................] - ETA: 3:05 - loss: 0.965 121/1016 [==>...........................] - ETA: 3:04 - loss: 0.963 122/1016 [==>...........................] - ETA: 3:02 - loss: 0.962 123/1016 [==>...........................] - ETA: 3:01 - loss: 0.960 124/1016 [==>...........................] - ETA: 3:00 - loss: 0.958 125/1016 [==>...........................] - ETA: 2:59 - loss: 0.957 126/1016 [==>...........................] - ETA: 2:57 - loss: 0.955 127/1016 [==>...........................] - ETA: 2:56 - loss: 0.954 128/1016 [==>...........................] - ETA: 2:55 - loss: 0.952 129/1016 [==>...........................] - ETA: 2:54 - loss: 0.951 130/1016 [==>...........................] - ETA: 2:53 - loss: 0.949 131/1016 [==>...........................] - ETA: 2:51 - loss: 0.948 132/1016 [==>...........................] - ETA: 2:50 - loss: 0.947 133/1016 [==>...........................] - ETA: 2:49 - loss: 0.946 134/1016 [==>...........................] - ETA: 2:48 - loss: 0.944 135/1016 [==>...........................] - ETA: 2:47 - loss: 0.943 136/1016 [===>..........................] - ETA: 2:46 - loss: 0.942 137/1016 [===>..........................] - ETA: 2:45 - loss: 0.941 138/1016 [===>..........................] - ETA: 2:44 - loss: 0.940 139/1016 [===>..........................] - ETA: 2:43 - loss: 0.938 140/1016 [===>..........................] - ETA: 2:42 - loss: 0.937 141/1016 [===>..........................] - ETA: 2:41 - loss: 0.935 142/1016 [===>..........................] - ETA: 2:40 - loss: 0.934 143/1016 [===>..........................] - ETA: 2:39 - loss: 0.933 144/1016 [===>..........................] - ETA: 2:38 - loss: 0.932 145/1016 [===>..........................] - ETA: 2:37 - loss: 0.930 146/1016 [===>..........................] - ETA: 2:36 - loss: 0.929 147/1016 [===>..........................] - ETA: 2:35 - loss: 0.928 148/1016 [===>..........................] - ETA: 2:34 - loss: 0.927 149/1016 [===>..........................] - ETA: 2:33 - loss: 0.926 150/1016 [===>..........................] - ETA: 2:32 - loss: 0.924 151/1016 [===>..........................] - ETA: 2:31 - loss: 0.923 152/1016 [===>..........................] - ETA: 2:31 - loss: 0.922 153/1016 [===>..........................] - ETA: 2:30 - loss: 0.921 154/1016 [===>..........................] - ETA: 2:29 - loss: 0.920 155/1016 [===>..........................] - ETA: 2:28 - loss: 0.919 156/1016 [===>..........................] - ETA: 2:27 - loss: 0.918 157/1016 [===>..........................] - ETA: 2:26 - loss: 0.917 158/1016 [===>..........................] - ETA: 2:26 - loss: 0.916 159/1016 [===>..........................] - ETA: 2:25 - loss: 0.916 160/1016 [===>..........................] - ETA: 2:24 - loss: 0.915 162/1016 [===>..........................] - ETA: 2:22 - loss: 0.913 163/1016 [===>..........................] - ETA: 2:22 - loss: 0.912 164/1016 [===>..........................] - ETA: 2:21 - loss: 0.911 165/1016 [===>..........................] - ETA: 2:20 - loss: 0.910 166/1016 [===>..........................] - ETA: 2:20 - loss: 0.909 167/1016 [===>..........................] - ETA: 2:19 - loss: 0.908 168/1016 [===>..........................] - ETA: 2:18 - loss: 0.907 169/1016 [===>..........................] - ETA: 2:17 - loss: 0.907 170/1016 [====>.........................] - ETA: 2:17 - loss: 0.906 171/1016 [====>.........................] - ETA: 2:16 - loss: 0.905 172/1016 [====>.........................] - ETA: 2:15 - loss: 0.904 173/1016 [====>.........................] - ETA: 2:15 - loss: 0.903 174/1016 [====>.........................] - ETA: 2:14 - loss: 0.902 175/1016 [====>.........................] - ETA: 2:13 - loss: 0.902 176/1016 [====>.........................] - ETA: 2:13 - loss: 0.901 177/1016 [====>.........................] - ETA: 2:12 - loss: 0.900 178/1016 [====>.........................] - ETA: 2:11 - loss: 0.899 179/1016 [====>.........................] - ETA: 2:11 - loss: 0.898 180/1016 [====>.........................] - ETA: 2:10 - loss: 0.898 181/1016 [====>.........................] - ETA: 2:09 - loss: 0.897 182/1016 [====>.........................] - ETA: 2:09 - loss: 0.896 183/1016 [====>.........................] - ETA: 2:08 - loss: 0.895 184/1016 [====>.........................] - ETA: 2:08 - loss: 0.895 185/1016 [====>.........................] - ETA: 2:07 - loss: 0.894 187/1016 [====>.........................] - ETA: 2:06 - loss: 0.892 188/1016 [====>.........................] - ETA: 2:05 - loss: 0.892 189/1016 [====>.........................] - ETA: 2:05 - loss: 0.891 190/1016 [====>.........................] - ETA: 2:04 - loss: 0.890 191/1016 [====>.........................] - ETA: 2:03 - loss: 0.890 192/1016 [====>.........................] - ETA: 2:03 - loss: 0.889 193/1016 [====>.........................] - ETA: 2:02 - loss: 0.888 194/1016 [====>.........................] - ETA: 2:02 - loss: 0.888 195/1016 [====>.........................] - ETA: 2:01 - loss: 0.887 196/1016 [====>.........................] - ETA: 2:01 - loss: 0.887 197/1016 [====>.........................] - ETA: 2:00 - loss: 0.886 198/1016 [====>.........................] - ETA: 2:00 - loss: 0.886 199/1016 [====>.........................] - ETA: 1:59 - loss: 0.885 200/1016 [====>.........................] - ETA: 1:59 - loss: 0.884 201/1016 [====>.........................] - ETA: 1:58 - loss: 0.884 202/1016 [====>.........................] - ETA: 1:58 - loss: 0.883 203/1016 [====>.........................] - ETA: 1:57 - loss: 0.882 204/1016 [=====>........................] - ETA: 1:57 - loss: 0.882 205/1016 [=====>........................] - ETA: 1:56 - loss: 0.881 206/1016 [=====>........................] - ETA: 1:56 - loss: 0.881 207/1016 [=====>........................] - ETA: 1:55 - loss: 0.880 208/1016 [=====>........................] - ETA: 1:55 - loss: 0.879 210/1016 [=====>........................] - ETA: 1:54 - loss: 0.879 211/1016 [=====>........................] - ETA: 1:53 - loss: 0.878 212/1016 [=====>........................] - ETA: 1:53 - loss: 0.878 213/1016 [=====>........................] - ETA: 1:52 - loss: 0.877 214/1016 [=====>........................] - ETA: 1:52 - loss: 0.877 215/1016 [=====>........................] - ETA: 1:51 - loss: 0.876 216/1016 [=====>........................] - ETA: 1:51 - loss: 0.876 217/1016 [=====>........................] - ETA: 1:50 - loss: 0.875 218/1016 [=====>........................] - ETA: 1:50 - loss: 0.875 219/1016 [=====>........................] - ETA: 1:50 - loss: 0.874 220/1016 [=====>........................] - ETA: 1:49 - loss: 0.874 221/1016 [=====>........................] - ETA: 1:49 - loss: 0.873 222/1016 [=====>........................] - ETA: 1:48 - loss: 0.873 223/1016 [=====>........................] - ETA: 1:48 - loss: 0.872 224/1016 [=====>........................] - ETA: 1:47 - loss: 0.872 225/1016 [=====>........................] - ETA: 1:47 - loss: 0.871 226/1016 [=====>........................] - ETA: 1:46 - loss: 0.871 227/1016 [=====>........................] - ETA: 1:46 - loss: 0.870 228/1016 [=====>........................] - ETA: 1:46 - loss: 0.870 229/1016 [=====>........................] - ETA: 1:45 - loss: 0.869 230/1016 [=====>........................] - ETA: 1:45 - loss: 0.869 231/1016 [=====>........................] - ETA: 1:44 - loss: 0.868 232/1016 [=====>........................] - ETA: 1:44 - loss: 0.868 233/1016 [=====>........................] - ETA: 1:44 - loss: 0.867 234/1016 [=====>........................] - ETA: 1:43 - loss: 0.867 235/1016 [=====>........................] - ETA: 1:43 - loss: 0.867 236/1016 [=====>........................] - ETA: 1:42 - loss: 0.866 237/1016 [=====>........................] - ETA: 1:42 - loss: 0.866 238/1016 [======>.......................] - ETA: 1:42 - loss: 0.865 239/1016 [======>.......................] - ETA: 1:41 - loss: 0.865 240/1016 [======>.......................] - ETA: 1:41 - loss: 0.864 241/1016 [======>.......................] - ETA: 1:41 - loss: 0.864 242/1016 [======>.......................] - ETA: 1:40 - loss: 0.864 243/1016 [======>.......................] - ETA: 1:40 - loss: 0.863 244/1016 [======>.......................] - ETA: 1:39 - loss: 0.863 245/1016 [======>.......................] - ETA: 1:39 - loss: 0.863 246/1016 [======>.......................] - ETA: 1:39 - loss: 0.862 247/1016 [======>.......................] - ETA: 1:38 - loss: 0.862 248/1016 [======>.......................] - ETA: 1:38 - loss: 0.861 249/1016 [======>.......................] - ETA: 1:38 - loss: 0.861 250/1016 [======>.......................] - ETA: 1:37 - loss: 0.861 251/1016 [======>.......................] - ETA: 1:37 - loss: 0.860 252/1016 [======>.......................] - ETA: 1:37 - loss: 0.860 253/1016 [======>.......................] - ETA: 1:36 - loss: 0.860 254/1016 [======>.......................] - ETA: 1:36 - loss: 0.859 255/1016 [======>.......................] - ETA: 1:36 - loss: 0.859 256/1016 [======>.......................] - ETA: 1:35 - loss: 0.859 257/1016 [======>.......................] - ETA: 1:35 - loss: 0.858 258/1016 [======>.......................] - ETA: 1:34 - loss: 0.858 260/1016 [======>.......................] - ETA: 1:34 - loss: 0.857 261/1016 [======>.......................] - ETA: 1:33 - loss: 0.857 262/1016 [======>.......................] - ETA: 1:33 - loss: 0.857 263/1016 [======>.......................] - ETA: 1:33 - loss: 0.856 264/1016 [======>.......................] - ETA: 1:32 - loss: 0.856 265/1016 [======>.......................] - ETA: 1:32 - loss: 0.856 266/1016 [======>.......................] - ETA: 1:32 - loss: 0.855 267/1016 [======>.......................] - ETA: 1:32 - loss: 0.855 268/1016 [======>.......................] - ETA: 1:31 - loss: 0.855 269/1016 [======>.......................] - ETA: 1:31 - loss: 0.854 270/1016 [======>.......................] - ETA: 1:31 - loss: 0.854 271/1016 [=======>......................] - ETA: 1:30 - loss: 0.854 273/1016 [=======>......................] - ETA: 1:30 - loss: 0.853 274/1016 [=======>......................] - ETA: 1:29 - loss: 0.853 275/1016 [=======>......................] - ETA: 1:29 - loss: 0.853 276/1016 [=======>......................] - ETA: 1:29 - loss: 0.852 277/1016 [=======>......................] - ETA: 1:29 - loss: 0.852 278/1016 [=======>......................] - ETA: 1:28 - loss: 0.852 279/1016 [=======>......................] - ETA: 1:28 - loss: 0.852 280/1016 [=======>......................] - ETA: 1:28 - loss: 0.851 281/1016 [=======>......................] - ETA: 1:27 - loss: 0.851 282/1016 [=======>......................] - ETA: 1:27 - loss: 0.851 284/1016 [=======>......................] - ETA: 1:27 - loss: 0.850 285/1016 [=======>......................] - ETA: 1:26 - loss: 0.850 286/1016 [=======>......................] - ETA: 1:26 - loss: 0.850 287/1016 [=======>......................] - ETA: 1:26 - loss: 0.849 289/1016 [=======>......................] - ETA: 1:25 - loss: 0.849 290/1016 [=======>......................] - ETA: 1:25 - loss: 0.849 291/1016 [=======>......................] - ETA: 1:25 - loss: 0.848 292/1016 [=======>......................] - ETA: 1:24 - loss: 0.848 293/1016 [=======>......................] - ETA: 1:24 - loss: 0.848 294/1016 [=======>......................] - ETA: 1:24 - loss: 0.848 295/1016 [=======>......................] - ETA: 1:23 - loss: 0.847 296/1016 [=======>......................] - ETA: 1:23 - loss: 0.847 297/1016 [=======>......................] - ETA: 1:23 - loss: 0.847 298/1016 [=======>......................] - ETA: 1:23 - loss: 0.846 299/1016 [=======>......................] - ETA: 1:22 - loss: 0.846 300/1016 [=======>......................] - ETA: 1:22 - loss: 0.845 301/1016 [=======>......................] - ETA: 1:22 - loss: 0.845 302/1016 [=======>......................] - ETA: 1:22 - loss: 0.845 303/1016 [=======>......................] - ETA: 1:21 - loss: 0.845 304/1016 [=======>......................] - ETA: 1:21 - loss: 0.844 306/1016 [========>.....................] - ETA: 1:21 - loss: 0.844 308/1016 [========>.....................] - ETA: 1:20 - loss: 0.843 310/1016 [========>.....................] - ETA: 1:20 - loss: 0.842 312/1016 [========>.....................] - ETA: 1:19 - loss: 0.842 313/1016 [========>.....................] - ETA: 1:19 - loss: 0.842 314/1016 [========>.....................] - ETA: 1:37 - loss: 0.842 315/1016 [========>.....................] - ETA: 1:36 - loss: 0.841 316/1016 [========>.....................] - ETA: 1:36 - loss: 0.841 317/1016 [========>.....................] - ETA: 1:36 - loss: 0.841 318/1016 [========>.....................] - ETA: 1:35 - loss: 0.841 319/1016 [========>.....................] - ETA: 1:35 - loss: 0.841 320/1016 [========>.....................] - ETA: 1:35 - loss: 0.840 321/1016 [========>.....................] - ETA: 1:34 - loss: 0.840 322/1016 [========>.....................] - ETA: 1:34 - loss: 0.840 324/1016 [========>.....................] - ETA: 1:33 - loss: 0.840 325/1016 [========>.....................] - ETA: 1:33 - loss: 0.839 326/1016 [========>.....................] - ETA: 1:33 - loss: 0.839 327/1016 [========>.....................] - ETA: 1:32 - loss: 0.839 328/1016 [========>.....................] - ETA: 1:32 - loss: 0.839 329/1016 [========>.....................] - ETA: 1:32 - loss: 0.838 330/1016 [========>.....................] - ETA: 1:32 - loss: 0.838 331/1016 [========>.....................] - ETA: 1:31 - loss: 0.838 332/1016 [========>.....................] - ETA: 1:31 - loss: 0.838 333/1016 [========>.....................] - ETA: 1:31 - loss: 0.837 334/1016 [========>.....................] - ETA: 1:30 - loss: 0.837 335/1016 [========>.....................] - ETA: 1:30 - loss: 0.837 336/1016 [========>.....................] - ETA: 1:30 - loss: 0.836 337/1016 [========>.....................] - ETA: 1:29 - loss: 0.836 338/1016 [========>.....................] - ETA: 1:29 - loss: 0.836 339/1016 [=========>....................] - ETA: 1:29 - loss: 0.836 340/1016 [=========>....................] - ETA: 1:29 - loss: 0.835 341/1016 [=========>....................] - ETA: 1:28 - loss: 0.835 342/1016 [=========>....................] - ETA: 1:28 - loss: 0.835 344/1016 [=========>....................] - ETA: 1:27 - loss: 0.834 345/1016 [=========>....................] - ETA: 1:27 - loss: 0.834 346/1016 [=========>....................] - ETA: 1:27 - loss: 0.834 348/1016 [=========>....................] - ETA: 1:26 - loss: 0.834 349/1016 [=========>....................] - ETA: 1:26 - loss: 0.833 350/1016 [=========>....................] - ETA: 1:26 - loss: 0.833 351/1016 [=========>....................] - ETA: 1:26 - loss: 0.833 352/1016 [=========>....................] - ETA: 1:25 - loss: 0.833 353/1016 [=========>....................] - ETA: 1:25 - loss: 0.833 354/1016 [=========>....................] - ETA: 1:25 - loss: 0.832 355/1016 [=========>....................] - ETA: 1:24 - loss: 0.832 356/1016 [=========>....................] - ETA: 1:24 - loss: 0.832 357/1016 [=========>....................] - ETA: 1:24 - loss: 0.832 358/1016 [=========>....................] - ETA: 1:24 - loss: 0.831 359/1016 [=========>....................] - ETA: 1:23 - loss: 0.831 360/1016 [=========>....................] - ETA: 1:23 - loss: 0.831 361/1016 [=========>....................] - ETA: 1:23 - loss: 0.831 362/1016 [=========>....................] - ETA: 1:23 - loss: 0.831 363/1016 [=========>....................] - ETA: 1:22 - loss: 0.830 365/1016 [=========>....................] - ETA: 1:22 - loss: 0.830 366/1016 [=========>....................] - ETA: 1:22 - loss: 0.830 367/1016 [=========>....................] - ETA: 1:21 - loss: 0.830 368/1016 [=========>....................] - ETA: 1:21 - loss: 0.830 369/1016 [=========>....................] - ETA: 1:21 - loss: 0.829 370/1016 [=========>....................] - ETA: 1:21 - loss: 0.829 371/1016 [=========>....................] - ETA: 1:20 - loss: 0.829 372/1016 [=========>....................] - ETA: 1:20 - loss: 0.829 373/1016 [==========>...................] - ETA: 1:20 - loss: 0.829 374/1016 [==========>...................] - ETA: 1:20 - loss: 0.829 375/1016 [==========>...................] - ETA: 1:19 - loss: 0.828 376/1016 [==========>...................] - ETA: 1:19 - loss: 0.828 377/1016 [==========>...................] - ETA: 1:19 - loss: 0.828 378/1016 [==========>...................] - ETA: 1:19 - loss: 0.828 379/1016 [==========>...................] - ETA: 1:18 - loss: 0.827 380/1016 [==========>...................] - ETA: 1:18 - loss: 0.827 381/1016 [==========>...................] - ETA: 1:18 - loss: 0.827 382/1016 [==========>...................] - ETA: 1:18 - loss: 0.827 383/1016 [==========>...................] - ETA: 1:17 - loss: 0.827 384/1016 [==========>...................] - ETA: 1:17 - loss: 0.827 385/1016 [==========>...................] - ETA: 1:17 - loss: 0.827 386/1016 [==========>...................] - ETA: 1:17 - loss: 0.827 387/1016 [==========>...................] - ETA: 1:17 - loss: 0.827 388/1016 [==========>...................] - ETA: 1:16 - loss: 0.827 389/1016 [==========>...................] - ETA: 1:16 - loss: 0.827 390/1016 [==========>...................] - ETA: 1:16 - loss: 0.826 391/1016 [==========>...................] - ETA: 1:16 - loss: 0.826 392/1016 [==========>...................] - ETA: 1:15 - loss: 0.826 393/1016 [==========>...................] - ETA: 1:15 - loss: 0.826 394/1016 [==========>...................] - ETA: 1:15 - loss: 0.826 395/1016 [==========>...................] - ETA: 1:15 - loss: 0.826 396/1016 [==========>...................] - ETA: 1:14 - loss: 0.825 397/1016 [==========>...................] - ETA: 1:14 - loss: 0.825 398/1016 [==========>...................] - ETA: 1:14 - loss: 0.825 399/1016 [==========>...................] - ETA: 1:14 - loss: 0.825 401/1016 [==========>...................] - ETA: 1:13 - loss: 0.825 402/1016 [==========>...................] - ETA: 1:13 - loss: 0.824 403/1016 [==========>...................] - ETA: 1:13 - loss: 0.824 404/1016 [==========>...................] - ETA: 1:13 - loss: 0.824 405/1016 [==========>...................] - ETA: 1:12 - loss: 0.824 406/1016 [==========>...................] - ETA: 1:12 - loss: 0.824 407/1016 [===========>..................] - ETA: 1:12 - loss: 0.823 408/1016 [===========>..................] - ETA: 1:12 - loss: 0.823 409/1016 [===========>..................] - ETA: 1:12 - loss: 0.823 410/1016 [===========>..................] - ETA: 1:11 - loss: 0.823 411/1016 [===========>..................] - ETA: 1:11 - loss: 0.823 412/1016 [===========>..................] - ETA: 1:11 - loss: 0.823 413/1016 [===========>..................] - ETA: 1:11 - loss: 0.823 415/1016 [===========>..................] - ETA: 1:10 - loss: 0.823 416/1016 [===========>..................] - ETA: 1:10 - loss: 0.822 417/1016 [===========>..................] - ETA: 1:10 - loss: 0.822 418/1016 [===========>..................] - ETA: 1:10 - loss: 0.822 419/1016 [===========>..................] - ETA: 1:09 - loss: 0.822 420/1016 [===========>..................] - ETA: 1:09 - loss: 0.822 421/1016 [===========>..................] - ETA: 1:09 - loss: 0.822 422/1016 [===========>..................] - ETA: 1:09 - loss: 0.822 423/1016 [===========>..................] - ETA: 1:09 - loss: 0.822 424/1016 [===========>..................] - ETA: 1:08 - loss: 0.821 425/1016 [===========>..................] - ETA: 1:08 - loss: 0.821 426/1016 [===========>..................] - ETA: 1:08 - loss: 0.821 427/1016 [===========>..................] - ETA: 1:08 - loss: 0.821 428/1016 [===========>..................] - ETA: 1:08 - loss: 0.821 429/1016 [===========>..................] - ETA: 1:07 - loss: 0.821 430/1016 [===========>..................] - ETA: 1:07 - loss: 0.821 431/1016 [===========>..................] - ETA: 1:07 - loss: 0.821 432/1016 [===========>..................] - ETA: 1:07 - loss: 0.820 433/1016 [===========>..................] - ETA: 1:07 - loss: 0.820 434/1016 [===========>..................] - ETA: 1:06 - loss: 0.820 435/1016 [===========>..................] - ETA: 1:06 - loss: 0.820 436/1016 [===========>..................] - ETA: 1:06 - loss: 0.820 437/1016 [===========>..................] - ETA: 1:06 - loss: 0.820 438/1016 [===========>..................] - ETA: 1:06 - loss: 0.820 439/1016 [===========>..................] - ETA: 1:05 - loss: 0.820 440/1016 [===========>..................] - ETA: 1:05 - loss: 0.819 441/1016 [============>.................] - ETA: 1:05 - loss: 0.819 442/1016 [============>.................] - ETA: 1:05 - loss: 0.819 443/1016 [============>.................] - ETA: 1:05 - loss: 0.819 444/1016 [============>.................] - ETA: 1:05 - loss: 0.819 445/1016 [============>.................] - ETA: 1:04 - loss: 0.819 446/1016 [============>.................] - ETA: 1:04 - loss: 0.819 447/1016 [============>.................] - ETA: 1:04 - loss: 0.818 448/1016 [============>.................] - ETA: 1:04 - loss: 0.818 449/1016 [============>.................] - ETA: 1:04 - loss: 0.818 450/1016 [============>.................] - ETA: 1:03 - loss: 0.818 451/1016 [============>.................] - ETA: 1:03 - loss: 0.818 452/1016 [============>.................] - ETA: 1:03 - loss: 0.818 453/1016 [============>.................] - ETA: 1:03 - loss: 0.818 454/1016 [============>.................] - ETA: 1:03 - loss: 0.818 455/1016 [============>.................] - ETA: 1:02 - loss: 0.817 456/1016 [============>.................] - ETA: 1:02 - loss: 0.817 457/1016 [============>.................] - ETA: 1:02 - loss: 0.817 458/1016 [============>.................] - ETA: 1:02 - loss: 0.817 459/1016 [============>.................] - ETA: 1:02 - loss: 0.817 460/1016 [============>.................] - ETA: 1:02 - loss: 0.817 461/1016 [============>.................] - ETA: 1:01 - loss: 0.817 462/1016 [============>.................] - ETA: 1:01 - loss: 0.817 463/1016 [============>.................] - ETA: 1:01 - loss: 0.817 464/1016 [============>.................] - ETA: 1:01 - loss: 0.817 465/1016 [============>.................] - ETA: 1:01 - loss: 0.816 466/1016 [============>.................] - ETA: 1:00 - loss: 0.816 467/1016 [============>.................] - ETA: 1:00 - loss: 0.816 468/1016 [============>.................] - ETA: 1:00 - loss: 0.816 469/1016 [============>.................] - ETA: 1:00 - loss: 0.816 470/1016 [============>.................] - ETA: 1:00 - loss: 0.816 471/1016 [============>.................] - ETA: 1:00 - loss: 0.816 472/1016 [============>.................] - ETA: 59s - loss: 0.8162 473/1016 [============>.................] - ETA: 59s - loss: 0.8161 474/1016 [============>.................] - ETA: 59s - loss: 0.8160 475/1016 [=============>................] - ETA: 59s - loss: 0.8159 476/1016 [=============>................] - ETA: 59s - loss: 0.8158 477/1016 [=============>................] - ETA: 59s - loss: 0.8157 478/1016 [=============>................] - ETA: 58s - loss: 0.8156 479/1016 [=============>................] - ETA: 58s - loss: 0.8155 480/1016 [=============>................] - ETA: 58s - loss: 0.8154 481/1016 [=============>................] - ETA: 58s - loss: 0.8153 482/1016 [=============>................] - ETA: 58s - loss: 0.8152 483/1016 [=============>................] - ETA: 57s - loss: 0.8151 484/1016 [=============>................] - ETA: 57s - loss: 0.8150 485/1016 [=============>................] - ETA: 57s - loss: 0.8149 486/1016 [=============>................] - ETA: 57s - loss: 0.8148 487/1016 [=============>................] - ETA: 57s - loss: 0.8146 488/1016 [=============>................] - ETA: 57s - loss: 0.8145 489/1016 [=============>................] - ETA: 56s - loss: 0.8144 490/1016 [=============>................] - ETA: 56s - loss: 0.8143 491/1016 [=============>................] - ETA: 56s - loss: 0.8141 492/1016 [=============>................] - ETA: 56s - loss: 0.8140 493/1016 [=============>................] - ETA: 56s - loss: 0.8140 494/1016 [=============>................] - ETA: 56s - loss: 0.8139 495/1016 [=============>................] - ETA: 55s - loss: 0.8138 496/1016 [=============>................] - ETA: 55s - loss: 0.8137 497/1016 [=============>................] - ETA: 55s - loss: 0.8136 498/1016 [=============>................] - ETA: 55s - loss: 0.8134 499/1016 [=============>................] - ETA: 55s - loss: 0.8133 500/1016 [=============>................] - ETA: 55s - loss: 0.8132 501/1016 [=============>................] - ETA: 55s - loss: 0.8132 502/1016 [=============>................] - ETA: 54s - loss: 0.8131 503/1016 [=============>................] - ETA: 54s - loss: 0.8130 504/1016 [=============>................] - ETA: 54s - loss: 0.8129 505/1016 [=============>................] - ETA: 54s - loss: 0.8128 506/1016 [=============>................] - ETA: 54s - loss: 0.8128 507/1016 [=============>................] - ETA: 54s - loss: 0.8127 508/1016 [==============>...............] - ETA: 53s - loss: 0.8126 509/1016 [==============>...............] - ETA: 53s - loss: 0.8125 510/1016 [==============>...............] - ETA: 53s - loss: 0.8124 511/1016 [==============>...............] - ETA: 53s - loss: 0.8124 512/1016 [==============>...............] - ETA: 53s - loss: 0.8122 513/1016 [==============>...............] - ETA: 53s - loss: 0.8121 514/1016 [==============>...............] - ETA: 52s - loss: 0.8121 515/1016 [==============>...............] - ETA: 52s - loss: 0.8120 516/1016 [==============>...............] - ETA: 52s - loss: 0.8119 517/1016 [==============>...............] - ETA: 52s - loss: 0.8118 518/1016 [==============>...............] - ETA: 52s - loss: 0.8117 519/1016 [==============>...............] - ETA: 52s - loss: 0.8115 520/1016 [==============>...............] - ETA: 52s - loss: 0.8115 521/1016 [==============>...............] - ETA: 51s - loss: 0.8114 522/1016 [==============>...............] - ETA: 51s - loss: 0.8113 524/1016 [==============>...............] - ETA: 51s - loss: 0.8111 525/1016 [==============>...............] - ETA: 51s - loss: 0.8110 526/1016 [==============>...............] - ETA: 51s - loss: 0.8109 527/1016 [==============>...............] - ETA: 50s - loss: 0.8108 528/1016 [==============>...............] - ETA: 50s - loss: 0.8107 529/1016 [==============>...............] - ETA: 50s - loss: 0.8107 530/1016 [==============>...............] - ETA: 50s - loss: 0.8106 531/1016 [==============>...............] - ETA: 50s - loss: 0.8105 532/1016 [==============>...............] - ETA: 50s - loss: 0.8103 533/1016 [==============>...............] - ETA: 50s - loss: 0.8103 534/1016 [==============>...............] - ETA: 49s - loss: 0.8103 535/1016 [==============>...............] - ETA: 49s - loss: 0.8102 536/1016 [==============>...............] - ETA: 49s - loss: 0.8100 537/1016 [==============>...............] - ETA: 49s - loss: 0.8100 538/1016 [==============>...............] - ETA: 49s - loss: 0.8099 540/1016 [==============>...............] - ETA: 49s - loss: 0.8097 541/1016 [==============>...............] - ETA: 48s - loss: 0.8097 542/1016 [===============>..............] - ETA: 48s - loss: 0.8096 543/1016 [===============>..............] - ETA: 48s - loss: 0.8095 544/1016 [===============>..............] - ETA: 48s - loss: 0.8095 545/1016 [===============>..............] - ETA: 48s - loss: 0.8093 546/1016 [===============>..............] - ETA: 48s - loss: 0.8092 548/1016 [===============>..............] - ETA: 47s - loss: 0.8090 549/1016 [===============>..............] - ETA: 47s - loss: 0.8088 550/1016 [===============>..............] - ETA: 47s - loss: 0.8087 551/1016 [===============>..............] - ETA: 47s - loss: 0.8085 552/1016 [===============>..............] - ETA: 47s - loss: 0.8085 553/1016 [===============>..............] - ETA: 47s - loss: 0.8083 554/1016 [===============>..............] - ETA: 46s - loss: 0.8082 555/1016 [===============>..............] - ETA: 46s - loss: 0.8081 556/1016 [===============>..............] - ETA: 46s - loss: 0.8080 557/1016 [===============>..............] - ETA: 46s - loss: 0.8079 558/1016 [===============>..............] - ETA: 46s - loss: 0.8079 559/1016 [===============>..............] - ETA: 46s - loss: 0.8078 560/1016 [===============>..............] - ETA: 46s - loss: 0.8077 561/1016 [===============>..............] - ETA: 46s - loss: 0.8076 562/1016 [===============>..............] - ETA: 45s - loss: 0.8075 563/1016 [===============>..............] - ETA: 45s - loss: 0.8074 564/1016 [===============>..............] - ETA: 45s - loss: 0.8073 565/1016 [===============>..............] - ETA: 45s - loss: 0.8072 566/1016 [===============>..............] - ETA: 45s - loss: 0.8071 567/1016 [===============>..............] - ETA: 45s - loss: 0.8070 568/1016 [===============>..............] - ETA: 45s - loss: 0.8069 569/1016 [===============>..............] - ETA: 44s - loss: 0.8068 570/1016 [===============>..............] - ETA: 44s - loss: 0.8067 571/1016 [===============>..............] - ETA: 44s - loss: 0.8067 572/1016 [===============>..............] - ETA: 44s - loss: 0.8066 573/1016 [===============>..............] - ETA: 44s - loss: 0.8065 574/1016 [===============>..............] - ETA: 44s - loss: 0.8065 575/1016 [===============>..............] - ETA: 44s - loss: 0.8065 576/1016 [================>.............] - ETA: 43s - loss: 0.8064 577/1016 [================>.............] - ETA: 43s - loss: 0.8063 578/1016 [================>.............] - ETA: 43s - loss: 0.8062 579/1016 [================>.............] - ETA: 43s - loss: 0.8061 580/1016 [================>.............] - ETA: 43s - loss: 0.8060 581/1016 [================>.............] - ETA: 43s - loss: 0.8059 583/1016 [================>.............] - ETA: 43s - loss: 0.8057 584/1016 [================>.............] - ETA: 42s - loss: 0.8056 585/1016 [================>.............] - ETA: 42s - loss: 0.8055 586/1016 [================>.............] - ETA: 42s - loss: 0.8053 587/1016 [================>.............] - ETA: 42s - loss: 0.8053 588/1016 [================>.............] - ETA: 42s - loss: 0.8053 589/1016 [================>.............] - ETA: 42s - loss: 0.8052 590/1016 [================>.............] - ETA: 42s - loss: 0.8051 591/1016 [================>.............] - ETA: 41s - loss: 0.8050 592/1016 [================>.............] - ETA: 41s - loss: 0.8048 593/1016 [================>.............] - ETA: 41s - loss: 0.8047 594/1016 [================>.............] - ETA: 41s - loss: 0.8046 595/1016 [================>.............] - ETA: 41s - loss: 0.8045 596/1016 [================>.............] - ETA: 41s - loss: 0.8044 597/1016 [================>.............] - ETA: 41s - loss: 0.8045 598/1016 [================>.............] - ETA: 41s - loss: 0.8044 599/1016 [================>.............] - ETA: 40s - loss: 0.8042 600/1016 [================>.............] - ETA: 40s - loss: 0.8041 601/1016 [================>.............] - ETA: 40s - loss: 0.8040 602/1016 [================>.............] - ETA: 40s - loss: 0.8040 603/1016 [================>.............] - ETA: 40s - loss: 0.8040 604/1016 [================>.............] - ETA: 40s - loss: 0.8039 605/1016 [================>.............] - ETA: 40s - loss: 0.8038 606/1016 [================>.............] - ETA: 40s - loss: 0.8037 608/1016 [================>.............] - ETA: 39s - loss: 0.8036 609/1016 [================>.............] - ETA: 39s - loss: 0.8036 610/1016 [=================>............] - ETA: 39s - loss: 0.8035 611/1016 [=================>............] - ETA: 39s - loss: 0.8035 612/1016 [=================>............] - ETA: 39s - loss: 0.8035 613/1016 [=================>............] - ETA: 39s - loss: 0.8035 614/1016 [=================>............] - ETA: 39s - loss: 0.8035 616/1016 [=================>............] - ETA: 38s - loss: 0.8034 617/1016 [=================>............] - ETA: 38s - loss: 0.8032 619/1016 [=================>............] - ETA: 38s - loss: 0.8032 621/1016 [=================>............] - ETA: 38s - loss: 0.8031 623/1016 [=================>............] - ETA: 37s - loss: 0.8030 625/1016 [=================>............] - ETA: 37s - loss: 0.8029 627/1016 [=================>............] - ETA: 42s - loss: 0.8028 628/1016 [=================>............] - ETA: 42s - loss: 0.8028 629/1016 [=================>............] - ETA: 42s - loss: 0.8028 630/1016 [=================>............] - ETA: 41s - loss: 0.8028 631/1016 [=================>............] - ETA: 41s - loss: 0.8027 632/1016 [=================>............] - ETA: 41s - loss: 0.8027 633/1016 [=================>............] - ETA: 41s - loss: 0.8026 634/1016 [=================>............] - ETA: 41s - loss: 0.8025 635/1016 [=================>............] - ETA: 41s - loss: 0.8025 636/1016 [=================>............] - ETA: 41s - loss: 0.8023 637/1016 [=================>............] - ETA: 40s - loss: 0.8023 638/1016 [=================>............] - ETA: 40s - loss: 0.8023 639/1016 [=================>............] - ETA: 40s - loss: 0.8022 640/1016 [=================>............] - ETA: 40s - loss: 0.8021 641/1016 [=================>............] - ETA: 40s - loss: 0.8021 642/1016 [=================>............] - ETA: 40s - loss: 0.8021 643/1016 [=================>............] - ETA: 40s - loss: 0.8021 644/1016 [==================>...........] - ETA: 39s - loss: 0.8020 645/1016 [==================>...........] - ETA: 39s - loss: 0.8020 646/1016 [==================>...........] - ETA: 39s - loss: 0.8019 647/1016 [==================>...........] - ETA: 39s - loss: 0.8019 648/1016 [==================>...........] - ETA: 39s - loss: 0.8019 649/1016 [==================>...........] - ETA: 39s - loss: 0.8017 650/1016 [==================>...........] - ETA: 39s - loss: 0.8017 651/1016 [==================>...........] - ETA: 38s - loss: 0.8016 652/1016 [==================>...........] - ETA: 38s - loss: 0.8016 653/1016 [==================>...........] - ETA: 38s - loss: 0.8015 654/1016 [==================>...........] - ETA: 38s - loss: 0.8015 655/1016 [==================>...........] - ETA: 38s - loss: 0.8015 656/1016 [==================>...........] - ETA: 38s - loss: 0.8015 657/1016 [==================>...........] - ETA: 38s - loss: 0.8015 658/1016 [==================>...........] - ETA: 38s - loss: 0.8014 659/1016 [==================>...........] - ETA: 37s - loss: 0.8013 660/1016 [==================>...........] - ETA: 37s - loss: 0.8013 661/1016 [==================>...........] - ETA: 37s - loss: 0.8012 663/1016 [==================>...........] - ETA: 37s - loss: 0.8011 664/1016 [==================>...........] - ETA: 37s - loss: 0.8010 665/1016 [==================>...........] - ETA: 37s - loss: 0.8010 666/1016 [==================>...........] - ETA: 36s - loss: 0.8010 667/1016 [==================>...........] - ETA: 36s - loss: 0.8009 668/1016 [==================>...........] - ETA: 36s - loss: 0.8008 669/1016 [==================>...........] - ETA: 36s - loss: 0.8008 670/1016 [==================>...........] - ETA: 36s - loss: 0.8008 671/1016 [==================>...........] - ETA: 36s - loss: 0.8007 672/1016 [==================>...........] - ETA: 36s - loss: 0.8007 673/1016 [==================>...........] - ETA: 36s - loss: 0.8007 674/1016 [==================>...........] - ETA: 35s - loss: 0.8006 675/1016 [==================>...........] - ETA: 35s - loss: 0.8006 676/1016 [==================>...........] - ETA: 35s - loss: 0.8005 677/1016 [==================>...........] - ETA: 35s - loss: 0.8005 678/1016 [===================>..........] - ETA: 35s - loss: 0.8004 679/1016 [===================>..........] - ETA: 35s - loss: 0.8004 680/1016 [===================>..........] - ETA: 35s - loss: 0.8004 681/1016 [===================>..........] - ETA: 35s - loss: 0.8003 682/1016 [===================>..........] - ETA: 34s - loss: 0.8002 683/1016 [===================>..........] - ETA: 34s - loss: 0.8002 684/1016 [===================>..........] - ETA: 34s - loss: 0.8000 685/1016 [===================>..........] - ETA: 34s - loss: 0.8001 686/1016 [===================>..........] - ETA: 34s - loss: 0.8000 688/1016 [===================>..........] - ETA: 34s - loss: 0.8000 689/1016 [===================>..........] - ETA: 33s - loss: 0.7999 690/1016 [===================>..........] - ETA: 33s - loss: 0.7999 691/1016 [===================>..........] - ETA: 33s - loss: 0.7998 692/1016 [===================>..........] - ETA: 33s - loss: 0.7998 693/1016 [===================>..........] - ETA: 33s - loss: 0.7998 694/1016 [===================>..........] - ETA: 33s - loss: 0.7997 695/1016 [===================>..........] - ETA: 33s - loss: 0.7997 696/1016 [===================>..........] - ETA: 33s - loss: 0.7996 697/1016 [===================>..........] - ETA: 32s - loss: 0.7996 698/1016 [===================>..........] - ETA: 32s - loss: 0.7995 699/1016 [===================>..........] - ETA: 32s - loss: 0.7994 700/1016 [===================>..........] - ETA: 32s - loss: 0.7994 701/1016 [===================>..........] - ETA: 32s - loss: 0.7994 702/1016 [===================>..........] - ETA: 32s - loss: 0.7993 703/1016 [===================>..........] - ETA: 32s - loss: 0.7993 704/1016 [===================>..........] - ETA: 32s - loss: 0.7992 705/1016 [===================>..........] - ETA: 31s - loss: 0.7992 706/1016 [===================>..........] - ETA: 31s - loss: 0.7991 707/1016 [===================>..........] - ETA: 31s - loss: 0.7991 708/1016 [===================>..........] - ETA: 31s - loss: 0.7991 709/1016 [===================>..........] - ETA: 31s - loss: 0.7990 710/1016 [===================>..........] - ETA: 31s - loss: 0.7990 711/1016 [===================>..........] - ETA: 31s - loss: 0.7990 712/1016 [====================>.........] - ETA: 31s - loss: 0.7989 713/1016 [====================>.........] - ETA: 30s - loss: 0.7988 714/1016 [====================>.........] - ETA: 30s - loss: 0.7987 715/1016 [====================>.........] - ETA: 30s - loss: 0.7987 716/1016 [====================>.........] - ETA: 30s - loss: 0.7987 717/1016 [====================>.........] - ETA: 30s - loss: 0.7986 718/1016 [====================>.........] - ETA: 30s - loss: 0.7986 719/1016 [====================>.........] - ETA: 30s - loss: 0.7985 720/1016 [====================>.........] - ETA: 30s - loss: 0.7985 722/1016 [====================>.........] - ETA: 29s - loss: 0.7984 723/1016 [====================>.........] - ETA: 29s - loss: 0.7983 724/1016 [====================>.........] - ETA: 29s - loss: 0.7983 725/1016 [====================>.........] - ETA: 29s - loss: 0.7982 726/1016 [====================>.........] - ETA: 29s - loss: 0.7982 727/1016 [====================>.........] - ETA: 29s - loss: 0.7982 728/1016 [====================>.........] - ETA: 29s - loss: 0.7981 729/1016 [====================>.........] - ETA: 29s - loss: 0.7980 730/1016 [====================>.........] - ETA: 28s - loss: 0.7980 731/1016 [====================>.........] - ETA: 28s - loss: 0.7979 732/1016 [====================>.........] - ETA: 28s - loss: 0.7980 733/1016 [====================>.........] - ETA: 28s - loss: 0.7979 734/1016 [====================>.........] - ETA: 28s - loss: 0.7979 736/1016 [====================>.........] - ETA: 28s - loss: 0.7978 737/1016 [====================>.........] - ETA: 28s - loss: 0.7978 739/1016 [====================>.........] - ETA: 27s - loss: 0.7976 740/1016 [====================>.........] - ETA: 27s - loss: 0.7976 741/1016 [====================>.........] - ETA: 27s - loss: 0.7976 742/1016 [====================>.........] - ETA: 27s - loss: 0.7975 743/1016 [====================>.........] - ETA: 27s - loss: 0.7975 744/1016 [====================>.........] - ETA: 27s - loss: 0.7974 745/1016 [====================>.........] - ETA: 27s - loss: 0.7973 746/1016 [=====================>........] - ETA: 27s - loss: 0.7973 747/1016 [=====================>........] - ETA: 26s - loss: 0.7972 748/1016 [=====================>........] - ETA: 26s - loss: 0.7972 749/1016 [=====================>........] - ETA: 26s - loss: 0.7972 750/1016 [=====================>........] - ETA: 26s - loss: 0.7972 751/1016 [=====================>........] - ETA: 26s - loss: 0.7971 752/1016 [=====================>........] - ETA: 26s - loss: 0.7971 753/1016 [=====================>........] - ETA: 26s - loss: 0.7971 754/1016 [=====================>........] - ETA: 26s - loss: 0.7970 755/1016 [=====================>........] - ETA: 25s - loss: 0.7970 756/1016 [=====================>........] - ETA: 25s - loss: 0.7969 757/1016 [=====================>........] - ETA: 25s - loss: 0.7969 758/1016 [=====================>........] - ETA: 25s - loss: 0.7968 759/1016 [=====================>........] - ETA: 25s - loss: 0.7969 760/1016 [=====================>........] - ETA: 25s - loss: 0.7969 761/1016 [=====================>........] - ETA: 25s - loss: 0.7969 762/1016 [=====================>........] - ETA: 25s - loss: 0.7968 763/1016 [=====================>........] - ETA: 25s - loss: 0.7968 764/1016 [=====================>........] - ETA: 24s - loss: 0.7967 765/1016 [=====================>........] - ETA: 24s - loss: 0.7967 766/1016 [=====================>........] - ETA: 24s - loss: 0.7966 767/1016 [=====================>........] - ETA: 24s - loss: 0.7965 768/1016 [=====================>........] - ETA: 24s - loss: 0.7965 769/1016 [=====================>........] - ETA: 24s - loss: 0.7965 770/1016 [=====================>........] - ETA: 24s - loss: 0.7964 772/1016 [=====================>........] - ETA: 24s - loss: 0.7963 773/1016 [=====================>........] - ETA: 23s - loss: 0.7962 775/1016 [=====================>........] - ETA: 23s - loss: 0.7961 776/1016 [=====================>........] - ETA: 23s - loss: 0.7960 777/1016 [=====================>........] - ETA: 23s - loss: 0.7960 779/1016 [======================>.......] - ETA: 23s - loss: 0.7959 780/1016 [======================>.......] - ETA: 23s - loss: 0.7958 781/1016 [======================>.......] - ETA: 23s - loss: 0.7958 782/1016 [======================>.......] - ETA: 22s - loss: 0.7958 783/1016 [======================>.......] - ETA: 22s - loss: 0.7957 784/1016 [======================>.......] - ETA: 22s - loss: 0.7956 785/1016 [======================>.......] - ETA: 22s - loss: 0.7956 786/1016 [======================>.......] - ETA: 22s - loss: 0.7956 787/1016 [======================>.......] - ETA: 22s - loss: 0.7956 788/1016 [======================>.......] - ETA: 22s - loss: 0.7956 789/1016 [======================>.......] - ETA: 22s - loss: 0.7956 790/1016 [======================>.......] - ETA: 22s - loss: 0.7955 791/1016 [======================>.......] - ETA: 21s - loss: 0.7955 792/1016 [======================>.......] - ETA: 21s - loss: 0.7954 793/1016 [======================>.......] - ETA: 21s - loss: 0.7954 794/1016 [======================>.......] - ETA: 21s - loss: 0.7953 795/1016 [======================>.......] - ETA: 21s - loss: 0.7952 796/1016 [======================>.......] - ETA: 21s - loss: 0.7952 797/1016 [======================>.......] - ETA: 21s - loss: 0.7952 798/1016 [======================>.......] - ETA: 21s - loss: 0.7951 799/1016 [======================>.......] - ETA: 21s - loss: 0.7951 800/1016 [======================>.......] - ETA: 20s - loss: 0.7951 802/1016 [======================>.......] - ETA: 20s - loss: 0.7950 803/1016 [======================>.......] - ETA: 20s - loss: 0.7949 804/1016 [======================>.......] - ETA: 20s - loss: 0.7949 805/1016 [======================>.......] - ETA: 20s - loss: 0.7949 806/1016 [======================>.......] - ETA: 20s - loss: 0.7948 807/1016 [======================>.......] - ETA: 20s - loss: 0.7948 808/1016 [======================>.......] - ETA: 20s - loss: 0.7947 809/1016 [======================>.......] - ETA: 19s - loss: 0.7947 810/1016 [======================>.......] - ETA: 19s - loss: 0.7946 811/1016 [======================>.......] - ETA: 19s - loss: 0.7946 812/1016 [======================>.......] - ETA: 19s - loss: 0.7945 813/1016 [=======================>......] - ETA: 19s - loss: 0.7945 814/1016 [=======================>......] - ETA: 19s - loss: 0.7944 815/1016 [=======================>......] - ETA: 19s - loss: 0.7944 816/1016 [=======================>......] - ETA: 19s - loss: 0.7944 817/1016 [=======================>......] - ETA: 19s - loss: 0.7943 818/1016 [=======================>......] - ETA: 18s - loss: 0.7943 819/1016 [=======================>......] - ETA: 18s - loss: 0.7942 820/1016 [=======================>......] - ETA: 18s - loss: 0.7941 821/1016 [=======================>......] - ETA: 18s - loss: 0.7941 822/1016 [=======================>......] - ETA: 18s - loss: 0.7940 823/1016 [=======================>......] - ETA: 18s - loss: 0.7940 824/1016 [=======================>......] - ETA: 18s - loss: 0.7940 825/1016 [=======================>......] - ETA: 18s - loss: 0.7940 826/1016 [=======================>......] - ETA: 18s - loss: 0.7940 827/1016 [=======================>......] - ETA: 18s - loss: 0.7940 828/1016 [=======================>......] - ETA: 17s - loss: 0.7939 829/1016 [=======================>......] - ETA: 17s - loss: 0.7939 830/1016 [=======================>......] - ETA: 17s - loss: 0.7938 831/1016 [=======================>......] - ETA: 17s - loss: 0.7938 832/1016 [=======================>......] - ETA: 17s - loss: 0.7938 833/1016 [=======================>......] - ETA: 17s - loss: 0.7937 834/1016 [=======================>......] - ETA: 17s - loss: 0.7936 835/1016 [=======================>......] - ETA: 17s - loss: 0.7935 836/1016 [=======================>......] - ETA: 17s - loss: 0.7935 837/1016 [=======================>......] - ETA: 16s - loss: 0.7935 838/1016 [=======================>......] - ETA: 16s - loss: 0.7934 839/1016 [=======================>......] - ETA: 16s - loss: 0.7933 840/1016 [=======================>......] - ETA: 16s - loss: 0.7933 841/1016 [=======================>......] - ETA: 16s - loss: 0.7933 842/1016 [=======================>......] - ETA: 16s - loss: 0.7933 843/1016 [=======================>......] - ETA: 16s - loss: 0.7933 844/1016 [=======================>......] - ETA: 16s - loss: 0.7932 845/1016 [=======================>......] - ETA: 16s - loss: 0.7932 846/1016 [=======================>......] - ETA: 16s - loss: 0.7932 847/1016 [========================>.....] - ETA: 15s - loss: 0.7932 848/1016 [========================>.....] - ETA: 15s - loss: 0.7931 849/1016 [========================>.....] - ETA: 15s - loss: 0.7930 850/1016 [========================>.....] - ETA: 15s - loss: 0.7930 851/1016 [========================>.....] - ETA: 15s - loss: 0.7929 852/1016 [========================>.....] - ETA: 15s - loss: 0.7928 853/1016 [========================>.....] - ETA: 15s - loss: 0.7928 854/1016 [========================>.....] - ETA: 15s - loss: 0.7927 856/1016 [========================>.....] - ETA: 15s - loss: 0.7926 857/1016 [========================>.....] - ETA: 14s - loss: 0.7926 858/1016 [========================>.....] - ETA: 14s - loss: 0.7926 859/1016 [========================>.....] - ETA: 14s - loss: 0.7925 860/1016 [========================>.....] - ETA: 14s - loss: 0.7925 861/1016 [========================>.....] - ETA: 14s - loss: 0.7924 862/1016 [========================>.....] - ETA: 14s - loss: 0.7924 863/1016 [========================>.....] - ETA: 14s - loss: 0.7923 864/1016 [========================>.....] - ETA: 14s - loss: 0.7923 865/1016 [========================>.....] - ETA: 14s - loss: 0.7922 866/1016 [========================>.....] - ETA: 14s - loss: 0.7922 867/1016 [========================>.....] - ETA: 13s - loss: 0.7922 868/1016 [========================>.....] - ETA: 13s - loss: 0.7921 869/1016 [========================>.....] - ETA: 13s - loss: 0.7921 870/1016 [========================>.....] - ETA: 13s - loss: 0.7921 871/1016 [========================>.....] - ETA: 13s - loss: 0.7921 872/1016 [========================>.....] - ETA: 13s - loss: 0.7920 873/1016 [========================>.....] - ETA: 13s - loss: 0.7920 874/1016 [========================>.....] - ETA: 13s - loss: 0.7920 875/1016 [========================>.....] - ETA: 13s - loss: 0.7919 876/1016 [========================>.....] - ETA: 13s - loss: 0.7919 877/1016 [========================>.....] - ETA: 12s - loss: 0.7919 878/1016 [========================>.....] - ETA: 12s - loss: 0.7918 879/1016 [========================>.....] - ETA: 12s - loss: 0.7918 880/1016 [========================>.....] - ETA: 12s - loss: 0.7918 881/1016 [=========================>....] - ETA: 12s - loss: 0.7917 882/1016 [=========================>....] - ETA: 12s - loss: 0.7917 883/1016 [=========================>....] - ETA: 12s - loss: 0.7917 884/1016 [=========================>....] - ETA: 12s - loss: 0.7917 885/1016 [=========================>....] - ETA: 12s - loss: 0.7917 886/1016 [=========================>....] - ETA: 12s - loss: 0.7917 887/1016 [=========================>....] - ETA: 11s - loss: 0.7917 888/1016 [=========================>....] - ETA: 11s - loss: 0.7917 889/1016 [=========================>....] - ETA: 11s - loss: 0.7916 890/1016 [=========================>....] - ETA: 11s - loss: 0.7916 892/1016 [=========================>....] - ETA: 11s - loss: 0.7915 893/1016 [=========================>....] - ETA: 11s - loss: 0.7915 894/1016 [=========================>....] - ETA: 11s - loss: 0.7914 895/1016 [=========================>....] - ETA: 11s - loss: 0.7914 896/1016 [=========================>....] - ETA: 11s - loss: 0.7913 897/1016 [=========================>....] - ETA: 10s - loss: 0.7913 898/1016 [=========================>....] - ETA: 10s - loss: 0.7913 899/1016 [=========================>....] - ETA: 10s - loss: 0.7913 900/1016 [=========================>....] - ETA: 10s - loss: 0.7913 901/1016 [=========================>....] - ETA: 10s - loss: 0.7912 902/1016 [=========================>....] - ETA: 10s - loss: 0.7911 903/1016 [=========================>....] - ETA: 10s - loss: 0.7911 904/1016 [=========================>....] - ETA: 10s - loss: 0.7911 905/1016 [=========================>....] - ETA: 10s - loss: 0.7911 906/1016 [=========================>....] - ETA: 10s - loss: 0.7910 907/1016 [=========================>....] - ETA: 9s - loss: 0.7911  908/1016 [=========================>....] - ETA: 9s - loss: 0.7910  909/1016 [=========================>....] - ETA: 9s - loss: 0.7909  910/1016 [=========================>....] - ETA: 9s - loss: 0.7909  911/1016 [=========================>....] - ETA: 9s - loss: 0.7909  912/1016 [=========================>....] - ETA: 9s - loss: 0.7908  913/1016 [=========================>....] - ETA: 9s - loss: 0.7908  914/1016 [=========================>....] - ETA: 9s - loss: 0.7908  915/1016 [==========================>...] - ETA: 9s - loss: 0.7908  916/1016 [==========================>...] - ETA: 9s - loss: 0.7907  917/1016 [==========================>...] - ETA: 9s - loss: 0.7907  918/1016 [==========================>...] - ETA: 8s - loss: 0.7907  919/1016 [==========================>...] - ETA: 8s - loss: 0.7907  920/1016 [==========================>...] - ETA: 8s - loss: 0.7906  921/1016 [==========================>...] - ETA: 8s - loss: 0.7906  922/1016 [==========================>...] - ETA: 8s - loss: 0.7906  923/1016 [==========================>...] - ETA: 8s - loss: 0.7906  924/1016 [==========================>...] - ETA: 8s - loss: 0.7906  925/1016 [==========================>...] - ETA: 8s - loss: 0.7906  926/1016 [==========================>...] - ETA: 8s - loss: 0.7906  927/1016 [==========================>...] - ETA: 8s - loss: 0.7906  928/1016 [==========================>...] - ETA: 7s - loss: 0.7905  929/1016 [==========================>...] - ETA: 7s - loss: 0.7905  931/1016 [==========================>...] - ETA: 7s - loss: 0.7905  933/1016 [==========================>...] - ETA: 7s - loss: 0.7905  935/1016 [==========================>...] - ETA: 7s - loss: 0.7904  937/1016 [==========================>...] - ETA: 7s - loss: 0.7903  939/1016 [==========================>...] - ETA: 6s - loss: 0.7903  940/1016 [==========================>...] - ETA: 7s - loss: 0.7902  941/1016 [==========================>...] - ETA: 7s - loss: 0.7902  942/1016 [==========================>...] - ETA: 7s - loss: 0.7902  943/1016 [==========================>...] - ETA: 7s - loss: 0.7902  944/1016 [==========================>...] - ETA: 7s - loss: 0.7901  945/1016 [==========================>...] - ETA: 7s - loss: 0.7901  946/1016 [==========================>...] - ETA: 6s - loss: 0.7901  947/1016 [==========================>...] - ETA: 6s - loss: 0.7901  948/1016 [==========================>...] - ETA: 6s - loss: 0.7900  949/1016 [===========================>..] - ETA: 6s - loss: 0.7900  950/1016 [===========================>..] - ETA: 6s - loss: 0.7900  951/1016 [===========================>..] - ETA: 6s - loss: 0.7899  952/1016 [===========================>..] - ETA: 6s - loss: 0.7899  953/1016 [===========================>..] - ETA: 6s - loss: 0.7898  954/1016 [===========================>..] - ETA: 6s - loss: 0.7898  955/1016 [===========================>..] - ETA: 5s - loss: 0.7898  956/1016 [===========================>..] - ETA: 5s - loss: 0.7898  957/1016 [===========================>..] - ETA: 5s - loss: 0.7897  958/1016 [===========================>..] - ETA: 5s - loss: 0.7896  959/1016 [===========================>..] - ETA: 5s - loss: 0.7896  960/1016 [===========================>..] - ETA: 5s - loss: 0.7896  961/1016 [===========================>..] - ETA: 5s - loss: 0.7896  962/1016 [===========================>..] - ETA: 5s - loss: 0.7896  963/1016 [===========================>..] - ETA: 5s - loss: 0.7895  964/1016 [===========================>..] - ETA: 5s - loss: 0.7895  965/1016 [===========================>..] - ETA: 4s - loss: 0.7894  966/1016 [===========================>..] - ETA: 4s - loss: 0.7894  967/1016 [===========================>..] - ETA: 4s - loss: 0.7893  968/1016 [===========================>..] - ETA: 4s - loss: 0.7893  969/1016 [===========================>..] - ETA: 4s - loss: 0.7892  970/1016 [===========================>..] - ETA: 4s - loss: 0.7892  971/1016 [===========================>..] - ETA: 4s - loss: 0.7892  972/1016 [===========================>..] - ETA: 4s - loss: 0.7892  973/1016 [===========================>..] - ETA: 4s - loss: 0.7892  974/1016 [===========================>..] - ETA: 4s - loss: 0.7892  975/1016 [===========================>..] - ETA: 3s - loss: 0.7892  976/1016 [===========================>..] - ETA: 3s - loss: 0.7892  977/1016 [===========================>..] - ETA: 3s - loss: 0.7891  978/1016 [===========================>..] - ETA: 3s - loss: 0.7891  979/1016 [===========================>..] - ETA: 3s - loss: 0.7891  980/1016 [===========================>..] - ETA: 3s - loss: 0.7890  981/1016 [===========================>..] - ETA: 3s - loss: 0.7889  982/1016 [===========================>..] - ETA: 3s - loss: 0.7889  983/1016 [============================>.] - ETA: 3s - loss: 0.7889  984/1016 [============================>.] - ETA: 3s - loss: 0.7888  985/1016 [============================>.] - ETA: 3s - loss: 0.7888  986/1016 [============================>.] - ETA: 2s - loss: 0.7888  987/1016 [============================>.] - ETA: 2s - loss: 0.7888  988/1016 [============================>.] - ETA: 2s - loss: 0.7887  989/1016 [============================>.] - ETA: 2s - loss: 0.7887  990/1016 [============================>.] - ETA: 2s - loss: 0.7887  991/1016 [============================>.] - ETA: 2s - loss: 0.7887  992/1016 [============================>.] - ETA: 2s - loss: 0.7886  993/1016 [============================>.] - ETA: 2s - loss: 0.7886  994/1016 [============================>.] - ETA: 2s - loss: 0.7886  995/1016 [============================>.] - ETA: 2s - loss: 0.7886  996/1016 [============================>.] - ETA: 1s - loss: 0.7885  997/1016 [============================>.] - ETA: 1s - loss: 0.7885  998/1016 [============================>.] - ETA: 1s - loss: 0.7885  999/1016 [============================>.] - ETA: 1s - loss: 0.7884 1000/1016 [============================>.] - ETA: 1s - loss: 0.7884 1001/1016 [============================>.] - ETA: 1s - loss: 0.7884 1002/1016 [============================>.] - ETA: 1s - loss: 0.7884 1003/1016 [============================>.] - ETA: 1s - loss: 0.7883 1004/1016 [============================>.] - ETA: 1s - loss: 0.7883 1005/1016 [============================>.] - ETA: 1s - loss: 0.7883 1006/1016 [============================>.] - ETA: 0s - loss: 0.7883 1007/1016 [============================>.] - ETA: 0s - loss: 0.7882 1008/1016 [============================>.] - ETA: 0s - loss: 0.7882 1009/1016 [============================>.] - ETA: 0s - loss: 0.7882 1010/1016 [============================>.] - ETA: 0s - loss: 0.7882 1011/1016 [============================>.] - ETA: 0s - loss: 0.7882 1012/1016 [============================>.] - ETA: 0s - loss: 0.7882 1013/1016 [============================>.] - ETA: 0s - loss: 0.7882 1014/1016 [============================>.] - ETA: 0s - loss: 0.7882 1015/1016 [============================>.] - ETA: 0s - loss: 0.7882 - mse: 0.3768 - mace: 25.2211
Epoch 00001: val_loss improved from inf to 0.76282, saving model to checkpoint.h5
1016/1016 [==============================] - 122s 120ms/step - loss: 0.7882 - mse: 0.3768 - mace: 25.2216 - val_loss: 0.7628 - val_mse: 0.3315 - val_mace: 24.4103

Epoch 00002: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 2/196
   1/1016 [..............................] - ETA: 1:26 - loss: 0.751   2/1016 [..............................] - ETA: 1:12 - loss: 0.743   3/1016 [..............................] - ETA: 1:08 - loss: 0.751   4/1016 [..............................] - ETA: 1:05 - loss: 0.761   5/1016 [..............................] - ETA: 1:03 - loss: 0.765   6/1016 [..............................] - ETA: 1:02 - loss: 0.761   7/1016 [..............................] - ETA: 1:02 - loss: 0.764   8/1016 [..............................] - ETA: 1:02 - loss: 0.765   9/1016 [..............................] - ETA: 1:01 - loss: 0.766  10/1016 [..............................] - ETA: 1:00 - loss: 0.767  11/1016 [..............................] - ETA: 1:00 - loss: 0.766  12/1016 [..............................] - ETA: 59s - loss: 0.7671  13/1016 [..............................] - ETA: 59s - loss: 0.7649  14/1016 [..............................] - ETA: 58s - loss: 0.7646  15/1016 [..............................] - ETA: 58s - loss: 0.7660  16/1016 [..............................] - ETA: 58s - loss: 0.7645  17/1016 [..............................] - ETA: 58s - loss: 0.7619  18/1016 [..............................] - ETA: 57s - loss: 0.7638  19/1016 [..............................] - ETA: 57s - loss: 0.7653  20/1016 [..............................] - ETA: 57s - loss: 0.7661  21/1016 [..............................] - ETA: 57s - loss: 0.7639  22/1016 [..............................] - ETA: 56s - loss: 0.7640  23/1016 [..............................] - ETA: 56s - loss: 0.7657  24/1016 [..............................] - ETA: 56s - loss: 0.7641  25/1016 [..............................] - ETA: 56s - loss: 0.7656  26/1016 [..............................] - ETA: 56s - loss: 0.7664  27/1016 [..............................] - ETA: 56s - loss: 0.7657  28/1016 [..............................] - ETA: 56s - loss: 0.7664  29/1016 [..............................] - ETA: 55s - loss: 0.7660  30/1016 [..............................] - ETA: 55s - loss: 0.7663  31/1016 [..............................] - ETA: 55s - loss: 0.7669  32/1016 [..............................] - ETA: 55s - loss: 0.7659  33/1016 [..............................] - ETA: 55s - loss: 0.7664  34/1016 [>.............................] - ETA: 55s - loss: 0.7665  35/1016 [>.............................] - ETA: 55s - loss: 0.7653  36/1016 [>.............................] - ETA: 55s - loss: 0.7662  37/1016 [>.............................] - ETA: 55s - loss: 0.7653  38/1016 [>.............................] - ETA: 55s - loss: 0.7642  39/1016 [>.............................] - ETA: 54s - loss: 0.7637  40/1016 [>.............................] - ETA: 54s - loss: 0.7648  41/1016 [>.............................] - ETA: 54s - loss: 0.7658  42/1016 [>.............................] - ETA: 54s - loss: 0.7655  43/1016 [>.............................] - ETA: 54s - loss: 0.7650  44/1016 [>.............................] - ETA: 54s - loss: 0.7653  45/1016 [>.............................] - ETA: 54s - loss: 0.7646  46/1016 [>.............................] - ETA: 54s - loss: 0.7658  47/1016 [>.............................] - ETA: 54s - loss: 0.7647  48/1016 [>.............................] - ETA: 54s - loss: 0.7641  49/1016 [>.............................] - ETA: 53s - loss: 0.7644  50/1016 [>.............................] - ETA: 53s - loss: 0.7629  51/1016 [>.............................] - ETA: 53s - loss: 0.7632  52/1016 [>.............................] - ETA: 53s - loss: 0.7636  53/1016 [>.............................] - ETA: 53s - loss: 0.7648  54/1016 [>.............................] - ETA: 53s - loss: 0.7646  55/1016 [>.............................] - ETA: 53s - loss: 0.7649  56/1016 [>.............................] - ETA: 53s - loss: 0.7650  57/1016 [>.............................] - ETA: 53s - loss: 0.7651  58/1016 [>.............................] - ETA: 53s - loss: 0.7650  59/1016 [>.............................] - ETA: 53s - loss: 0.7653  60/1016 [>.............................] - ETA: 53s - loss: 0.7649  61/1016 [>.............................] - ETA: 53s - loss: 0.7649  62/1016 [>.............................] - ETA: 53s - loss: 0.7653  63/1016 [>.............................] - ETA: 53s - loss: 0.7648  64/1016 [>.............................] - ETA: 53s - loss: 0.7652  65/1016 [>.............................] - ETA: 53s - loss: 0.7651  66/1016 [>.............................] - ETA: 52s - loss: 0.7652  67/1016 [>.............................] - ETA: 52s - loss: 0.7651  68/1016 [=>............................] - ETA: 52s - loss: 0.7656  69/1016 [=>............................] - ETA: 52s - loss: 0.7666  70/1016 [=>............................] - ETA: 52s - loss: 0.7658  71/1016 [=>............................] - ETA: 52s - loss: 0.7657  72/1016 [=>............................] - ETA: 52s - loss: 0.7654  73/1016 [=>............................] - ETA: 52s - loss: 0.7657  74/1016 [=>............................] - ETA: 52s - loss: 0.7656  75/1016 [=>............................] - ETA: 52s - loss: 0.7658  76/1016 [=>............................] - ETA: 52s - loss: 0.7662  77/1016 [=>............................] - ETA: 52s - loss: 0.7666  78/1016 [=>............................] - ETA: 52s - loss: 0.7665  79/1016 [=>............................] - ETA: 52s - loss: 0.7676  80/1016 [=>............................] - ETA: 51s - loss: 0.7673  81/1016 [=>............................] - ETA: 51s - loss: 0.7675  82/1016 [=>............................] - ETA: 51s - loss: 0.7675  83/1016 [=>............................] - ETA: 51s - loss: 0.7678  84/1016 [=>............................] - ETA: 51s - loss: 0.7677  85/1016 [=>............................] - ETA: 51s - loss: 0.7678  86/1016 [=>............................] - ETA: 51s - loss: 0.7681  87/1016 [=>............................] - ETA: 51s - loss: 0.7680  88/1016 [=>............................] - ETA: 51s - loss: 0.7673  89/1016 [=>............................] - ETA: 51s - loss: 0.7670  90/1016 [=>............................] - ETA: 51s - loss: 0.7666  91/1016 [=>............................] - ETA: 51s - loss: 0.7664  92/1016 [=>............................] - ETA: 51s - loss: 0.7665  93/1016 [=>............................] - ETA: 51s - loss: 0.7664  94/1016 [=>............................] - ETA: 50s - loss: 0.7663  95/1016 [=>............................] - ETA: 50s - loss: 0.7663  96/1016 [=>............................] - ETA: 50s - loss: 0.7661  97/1016 [=>............................] - ETA: 50s - loss: 0.7657  98/1016 [=>............................] - ETA: 50s - loss: 0.7660  99/1016 [=>............................] - ETA: 50s - loss: 0.7660 100/1016 [=>............................] - ETA: 50s - loss: 0.7663 101/1016 [=>............................] - ETA: 50s - loss: 0.7661 102/1016 [==>...........................] - ETA: 50s - loss: 0.7661 103/1016 [==>...........................] - ETA: 50s - loss: 0.7663 104/1016 [==>...........................] - ETA: 50s - loss: 0.7663 105/1016 [==>...........................] - ETA: 50s - loss: 0.7658 106/1016 [==>...........................] - ETA: 50s - loss: 0.7659 107/1016 [==>...........................] - ETA: 50s - loss: 0.7656 108/1016 [==>...........................] - ETA: 50s - loss: 0.7661 109/1016 [==>...........................] - ETA: 50s - loss: 0.7657 110/1016 [==>...........................] - ETA: 50s - loss: 0.7655 111/1016 [==>...........................] - ETA: 50s - loss: 0.7654 112/1016 [==>...........................] - ETA: 49s - loss: 0.7657 113/1016 [==>...........................] - ETA: 49s - loss: 0.7657 114/1016 [==>...........................] - ETA: 49s - loss: 0.7659 115/1016 [==>...........................] - ETA: 49s - loss: 0.7661 116/1016 [==>...........................] - ETA: 49s - loss: 0.7662 117/1016 [==>...........................] - ETA: 49s - loss: 0.7665 118/1016 [==>...........................] - ETA: 49s - loss: 0.7664 119/1016 [==>...........................] - ETA: 49s - loss: 0.7663 120/1016 [==>...........................] - ETA: 49s - loss: 0.7664 121/1016 [==>...........................] - ETA: 49s - loss: 0.7669 122/1016 [==>...........................] - ETA: 49s - loss: 0.7667 123/1016 [==>...........................] - ETA: 49s - loss: 0.7669 124/1016 [==>...........................] - ETA: 49s - loss: 0.7670 125/1016 [==>...........................] - ETA: 49s - loss: 0.7672 126/1016 [==>...........................] - ETA: 49s - loss: 0.7671 127/1016 [==>...........................] - ETA: 49s - loss: 0.7668 128/1016 [==>...........................] - ETA: 49s - loss: 0.7667 129/1016 [==>...........................] - ETA: 49s - loss: 0.7667 130/1016 [==>...........................] - ETA: 48s - loss: 0.7666 131/1016 [==>...........................] - ETA: 48s - loss: 0.7666 132/1016 [==>...........................] - ETA: 48s - loss: 0.7670 133/1016 [==>...........................] - ETA: 48s - loss: 0.7670 134/1016 [==>...........................] - ETA: 48s - loss: 0.7667 135/1016 [==>...........................] - ETA: 48s - loss: 0.7666 136/1016 [===>..........................] - ETA: 48s - loss: 0.7666 137/1016 [===>..........................] - ETA: 48s - loss: 0.7665 138/1016 [===>..........................] - ETA: 48s - loss: 0.7670 139/1016 [===>..........................] - ETA: 48s - loss: 0.7670 140/1016 [===>..........................] - ETA: 48s - loss: 0.7672 141/1016 [===>..........................] - ETA: 48s - loss: 0.7674 142/1016 [===>..........................] - ETA: 48s - loss: 0.7675 143/1016 [===>..........................] - ETA: 48s - loss: 0.7676 144/1016 [===>..........................] - ETA: 48s - loss: 0.7674 145/1016 [===>..........................] - ETA: 48s - loss: 0.7672 146/1016 [===>..........................] - ETA: 48s - loss: 0.7669 147/1016 [===>..........................] - ETA: 47s - loss: 0.7666 148/1016 [===>..........................] - ETA: 47s - loss: 0.7665 149/1016 [===>..........................] - ETA: 47s - loss: 0.7666 150/1016 [===>..........................] - ETA: 47s - loss: 0.7664 151/1016 [===>..........................] - ETA: 47s - loss: 0.7662 152/1016 [===>..........................] - ETA: 47s - loss: 0.7662 153/1016 [===>..........................] - ETA: 47s - loss: 0.7660 154/1016 [===>..........................] - ETA: 47s - loss: 0.7658 155/1016 [===>..........................] - ETA: 47s - loss: 0.7658 156/1016 [===>..........................] - ETA: 47s - loss: 0.7657 157/1016 [===>..........................] - ETA: 47s - loss: 0.7659 158/1016 [===>..........................] - ETA: 47s - loss: 0.7656 159/1016 [===>..........................] - ETA: 47s - loss: 0.7655 160/1016 [===>..........................] - ETA: 47s - loss: 0.7652 161/1016 [===>..........................] - ETA: 47s - loss: 0.7654 162/1016 [===>..........................] - ETA: 47s - loss: 0.7654 163/1016 [===>..........................] - ETA: 47s - loss: 0.7655 164/1016 [===>..........................] - ETA: 47s - loss: 0.7655 165/1016 [===>..........................] - ETA: 47s - loss: 0.7657 166/1016 [===>..........................] - ETA: 47s - loss: 0.7655 167/1016 [===>..........................] - ETA: 46s - loss: 0.7655 168/1016 [===>..........................] - ETA: 46s - loss: 0.7654 169/1016 [===>..........................] - ETA: 46s - loss: 0.7655 170/1016 [====>.........................] - ETA: 46s - loss: 0.7652 171/1016 [====>.........................] - ETA: 46s - loss: 0.7653 172/1016 [====>.........................] - ETA: 46s - loss: 0.7654 173/1016 [====>.........................] - ETA: 46s - loss: 0.7656 174/1016 [====>.........................] - ETA: 46s - loss: 0.7656 175/1016 [====>.........................] - ETA: 46s - loss: 0.7656 176/1016 [====>.........................] - ETA: 46s - loss: 0.7657 177/1016 [====>.........................] - ETA: 46s - loss: 0.7656 178/1016 [====>.........................] - ETA: 46s - loss: 0.7658 179/1016 [====>.........................] - ETA: 46s - loss: 0.7657 180/1016 [====>.........................] - ETA: 46s - loss: 0.7655 181/1016 [====>.........................] - ETA: 46s - loss: 0.7656 182/1016 [====>.........................] - ETA: 46s - loss: 0.7656 183/1016 [====>.........................] - ETA: 46s - loss: 0.7656 184/1016 [====>.........................] - ETA: 46s - loss: 0.7657 185/1016 [====>.........................] - ETA: 45s - loss: 0.7658 186/1016 [====>.........................] - ETA: 45s - loss: 0.7656 187/1016 [====>.........................] - ETA: 45s - loss: 0.7656 188/1016 [====>.........................] - ETA: 45s - loss: 0.7656 189/1016 [====>.........................] - ETA: 45s - loss: 0.7657 190/1016 [====>.........................] - ETA: 45s - loss: 0.7656 191/1016 [====>.........................] - ETA: 45s - loss: 0.7657 192/1016 [====>.........................] - ETA: 45s - loss: 0.7656 193/1016 [====>.........................] - ETA: 45s - loss: 0.7655 194/1016 [====>.........................] - ETA: 45s - loss: 0.7654 195/1016 [====>.........................] - ETA: 45s - loss: 0.7655 196/1016 [====>.........................] - ETA: 45s - loss: 0.7656 197/1016 [====>.........................] - ETA: 45s - loss: 0.7655 198/1016 [====>.........................] - ETA: 45s - loss: 0.7657 199/1016 [====>.........................] - ETA: 45s - loss: 0.7659 200/1016 [====>.........................] - ETA: 45s - loss: 0.7659 201/1016 [====>.........................] - ETA: 45s - loss: 0.7659 202/1016 [====>.........................] - ETA: 45s - loss: 0.7659 203/1016 [====>.........................] - ETA: 44s - loss: 0.7661 204/1016 [=====>........................] - ETA: 44s - loss: 0.7659 205/1016 [=====>........................] - ETA: 44s - loss: 0.7660 206/1016 [=====>........................] - ETA: 44s - loss: 0.7658 207/1016 [=====>........................] - ETA: 44s - loss: 0.7658 208/1016 [=====>........................] - ETA: 44s - loss: 0.7657 209/1016 [=====>........................] - ETA: 44s - loss: 0.7657 210/1016 [=====>........................] - ETA: 44s - loss: 0.7660 211/1016 [=====>........................] - ETA: 44s - loss: 0.7661 212/1016 [=====>........................] - ETA: 44s - loss: 0.7661 213/1016 [=====>........................] - ETA: 44s - loss: 0.7658 214/1016 [=====>........................] - ETA: 44s - loss: 0.7657 215/1016 [=====>........................] - ETA: 44s - loss: 0.7656 216/1016 [=====>........................] - ETA: 44s - loss: 0.7656 217/1016 [=====>........................] - ETA: 44s - loss: 0.7655 218/1016 [=====>........................] - ETA: 44s - loss: 0.7654 219/1016 [=====>........................] - ETA: 44s - loss: 0.7652 220/1016 [=====>........................] - ETA: 44s - loss: 0.7650 221/1016 [=====>........................] - ETA: 43s - loss: 0.7650 222/1016 [=====>........................] - ETA: 43s - loss: 0.7650 223/1016 [=====>........................] - ETA: 43s - loss: 0.7648 224/1016 [=====>........................] - ETA: 43s - loss: 0.7649 225/1016 [=====>........................] - ETA: 43s - loss: 0.7650 226/1016 [=====>........................] - ETA: 43s - loss: 0.7651 227/1016 [=====>........................] - ETA: 43s - loss: 0.7653 228/1016 [=====>........................] - ETA: 43s - loss: 0.7651 230/1016 [=====>........................] - ETA: 43s - loss: 0.7652 231/1016 [=====>........................] - ETA: 43s - loss: 0.7651 232/1016 [=====>........................] - ETA: 43s - loss: 0.7651 234/1016 [=====>........................] - ETA: 43s - loss: 0.7648 235/1016 [=====>........................] - ETA: 43s - loss: 0.7647 237/1016 [=====>........................] - ETA: 1:09 - loss: 0.764 238/1016 [======>.......................] - ETA: 1:09 - loss: 0.764 239/1016 [======>.......................] - ETA: 1:09 - loss: 0.764 240/1016 [======>.......................] - ETA: 1:08 - loss: 0.764 241/1016 [======>.......................] - ETA: 1:08 - loss: 0.764 242/1016 [======>.......................] - ETA: 1:08 - loss: 0.764 243/1016 [======>.......................] - ETA: 1:08 - loss: 0.764 244/1016 [======>.......................] - ETA: 1:08 - loss: 0.764 245/1016 [======>.......................] - ETA: 1:07 - loss: 0.764 246/1016 [======>.......................] - ETA: 1:07 - loss: 0.764 247/1016 [======>.......................] - ETA: 1:07 - loss: 0.765 248/1016 [======>.......................] - ETA: 1:07 - loss: 0.765 249/1016 [======>.......................] - ETA: 1:07 - loss: 0.765 250/1016 [======>.......................] - ETA: 1:06 - loss: 0.765 251/1016 [======>.......................] - ETA: 1:06 - loss: 0.765 252/1016 [======>.......................] - ETA: 1:06 - loss: 0.765 253/1016 [======>.......................] - ETA: 1:06 - loss: 0.765 254/1016 [======>.......................] - ETA: 1:06 - loss: 0.765 255/1016 [======>.......................] - ETA: 1:05 - loss: 0.765 256/1016 [======>.......................] - ETA: 1:05 - loss: 0.765 257/1016 [======>.......................] - ETA: 1:05 - loss: 0.765 258/1016 [======>.......................] - ETA: 1:05 - loss: 0.765 259/1016 [======>.......................] - ETA: 1:05 - loss: 0.765 260/1016 [======>.......................] - ETA: 1:05 - loss: 0.765 261/1016 [======>.......................] - ETA: 1:04 - loss: 0.765 262/1016 [======>.......................] - ETA: 1:04 - loss: 0.765 263/1016 [======>.......................] - ETA: 1:04 - loss: 0.765 264/1016 [======>.......................] - ETA: 1:04 - loss: 0.765 265/1016 [======>.......................] - ETA: 1:04 - loss: 0.765 266/1016 [======>.......................] - ETA: 1:04 - loss: 0.765 267/1016 [======>.......................] - ETA: 1:03 - loss: 0.765 268/1016 [======>.......................] - ETA: 1:03 - loss: 0.765 269/1016 [======>.......................] - ETA: 1:03 - loss: 0.765 270/1016 [======>.......................] - ETA: 1:03 - loss: 0.765 271/1016 [=======>......................] - ETA: 1:03 - loss: 0.765 272/1016 [=======>......................] - ETA: 1:03 - loss: 0.765 273/1016 [=======>......................] - ETA: 1:02 - loss: 0.765 274/1016 [=======>......................] - ETA: 1:02 - loss: 0.765 275/1016 [=======>......................] - ETA: 1:02 - loss: 0.765 276/1016 [=======>......................] - ETA: 1:02 - loss: 0.765 277/1016 [=======>......................] - ETA: 1:02 - loss: 0.765 278/1016 [=======>......................] - ETA: 1:02 - loss: 0.765 279/1016 [=======>......................] - ETA: 1:01 - loss: 0.765 280/1016 [=======>......................] - ETA: 1:01 - loss: 0.765 281/1016 [=======>......................] - ETA: 1:01 - loss: 0.765 282/1016 [=======>......................] - ETA: 1:01 - loss: 0.765 283/1016 [=======>......................] - ETA: 1:01 - loss: 0.765 284/1016 [=======>......................] - ETA: 1:01 - loss: 0.765 285/1016 [=======>......................] - ETA: 1:00 - loss: 0.765 286/1016 [=======>......................] - ETA: 1:00 - loss: 0.765 287/1016 [=======>......................] - ETA: 1:00 - loss: 0.765 288/1016 [=======>......................] - ETA: 1:00 - loss: 0.765 289/1016 [=======>......................] - ETA: 1:00 - loss: 0.765 290/1016 [=======>......................] - ETA: 1:00 - loss: 0.765 291/1016 [=======>......................] - ETA: 59s - loss: 0.7651 292/1016 [=======>......................] - ETA: 59s - loss: 0.7652 293/1016 [=======>......................] - ETA: 59s - loss: 0.7653 294/1016 [=======>......................] - ETA: 59s - loss: 0.7653 295/1016 [=======>......................] - ETA: 59s - loss: 0.7652 296/1016 [=======>......................] - ETA: 59s - loss: 0.7651 297/1016 [=======>......................] - ETA: 59s - loss: 0.7651 298/1016 [=======>......................] - ETA: 58s - loss: 0.7649 299/1016 [=======>......................] - ETA: 58s - loss: 0.7649 300/1016 [=======>......................] - ETA: 58s - loss: 0.7650 301/1016 [=======>......................] - 302/1016 [=======>......................] - ETA: 58s - loss: 0.7650 - mse: 0.33 303/1016 [=======>......................] - ETA: 58s - loss: 0.7649 - mse: 0.33 304/1016 [=======>.....1015/1016 [============================>.] - ETA: 0s - loss: 0.7652 - mse: 0.3338 - mace: 24.4849 
Epoch 00002: val_loss did not improve from 0.76282
1016/1016 [==============================] - 100s 99ms/step - loss: 0.7652 - mse: 0.3339 - mace: 24.4857 - val_loss: 0.7643 - val_mse: 0.3325 - val_mace: 24.4561

Epoch 00003: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 3/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7624 - mse: 0.3325 - mace: 24.3956  
Epoch 00003: val_loss improved from 0.76282 to 0.75509, saving model to checkpoint.h5
1016/1016 [==============================] - 107s 105ms/step - loss: 0.7623 - mse: 0.3325 - mace: 24.3950 - val_loss: 0.7551 - val_mse: 0.3256 - val_mace: 24.1630

Epoch 00004: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 4/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7560 - mse: 0.3285 - mace: 24.1906  
Epoch 00004: val_loss improved from 0.75509 to 0.74816, saving model to checkpoint.h5
1016/1016 [==============================] - 102s 100ms/step - loss: 0.7559 - mse: 0.3284 - mace: 24.1893 - val_loss: 0.7482 - val_mse: 0.3210 - val_mace: 23.9410

Epoch 00005: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 5/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7475 - mse: 0.3229 - mace: 23.9211  
Epoch 00005: val_loss improved from 0.74816 to 0.73921, saving model to checkpoint.h5
1016/1016 [==============================] - 102s 100ms/step - loss: 0.7475 - mse: 0.3230 - mace: 23.9212 - val_loss: 0.7392 - val_mse: 0.3135 - val_mace: 23.6547

Epoch 00006: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 6/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7404 - mse: 0.3184 - mace: 23.6925  
Epoch 00006: val_loss improved from 0.73921 to 0.73809, saving model to checkpoint.h5
1016/1016 [==============================] - 95s 93ms/step - loss: 0.7404 - mse: 0.3184 - mace: 23.6921 - val_loss: 0.7381 - val_mse: 0.3135 - val_mace: 23.6189

Epoch 00007: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 7/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7359 - mse: 0.3153 - mace: 23.5493  
Epoch 00007: val_loss improved from 0.73809 to 0.73749, saving model to checkpoint.h5
1016/1016 [==============================] - 111s 109ms/step - loss: 0.7359 - mse: 0.3153 - mace: 23.5493 - val_loss: 0.7375 - val_mse: 0.3137 - val_mace: 23.5996

Epoch 00008: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 8/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7314 - mse: 0.3123 - mace: 23.4045  
Epoch 00008: val_loss improved from 0.73749 to 0.73633, saving model to checkpoint.h5
1016/1016 [==============================] - 103s 101ms/step - loss: 0.7314 - mse: 0.3123 - mace: 23.4047 - val_loss: 0.7363 - val_mse: 0.3130 - val_mace: 23.5627

Epoch 00009: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 9/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7252 - mse: 0.3085 - mace: 23.2059  
Epoch 00009: val_loss did not improve from 0.73633
1016/1016 [==============================] - 100s 98ms/step - loss: 0.7252 - mse: 0.3085 - mace: 23.2055 - val_loss: 0.7475 - val_mse: 0.3234 - val_mace: 23.9188

Epoch 00010: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 10/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7218 - mse: 0.3057 - mace: 23.0964  
Epoch 00010: val_loss improved from 0.73633 to 0.73441, saving model to checkpoint.h5
1016/1016 [==============================] - 103s 102ms/step - loss: 0.7218 - mse: 0.3057 - mace: 23.0971 - val_loss: 0.7344 - val_mse: 0.3105 - val_mace: 23.5011

Epoch 00011: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 11/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7153 - mse: 0.3019 - mace: 22.8898  
Epoch 00011: val_loss did not improve from 0.73441
1016/1016 [==============================] - 106s 104ms/step - loss: 0.7153 - mse: 0.3020 - mace: 22.8907 - val_loss: 0.7403 - val_mse: 0.3162 - val_mace: 23.6912

Epoch 00012: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 12/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7136 - mse: 0.3005 - mace: 22.8367  
Epoch 00012: val_loss did not improve from 0.73441
1016/1016 [==============================] - 92s 91ms/step - loss: 0.7137 - mse: 0.3006 - mace: 22.8385 - val_loss: 0.7366 - val_mse: 0.3131 - val_mace: 23.5700

Epoch 00013: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 13/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7122 - mse: 0.2995 - mace: 22.7901  
Epoch 00013: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.7122 - mse: 0.2995 - mace: 22.7904 - val_loss: 0.7396 - val_mse: 0.3166 - val_mace: 23.6683

Epoch 00014: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 14/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7101 - mse: 0.2985 - mace: 22.7239  
Epoch 00014: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 99ms/step - loss: 0.7101 - mse: 0.2985 - mace: 22.7248 - val_loss: 0.7420 - val_mse: 0.3198 - val_mace: 23.7437

Epoch 00015: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 15/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7089 - mse: 0.2972 - mace: 22.6839  
Epoch 00015: val_loss did not improve from 0.73441
1016/1016 [==============================] - 108s 106ms/step - loss: 0.7089 - mse: 0.2972 - mace: 22.6842 - val_loss: 0.7438 - val_mse: 0.3215 - val_mace: 23.8010

Epoch 00016: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 16/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7076 - mse: 0.2965 - mace: 22.6439 
Epoch 00016: val_loss did not improve from 0.73441
1016/1016 [==============================] - 107s 105ms/step - loss: 0.7076 - mse: 0.2965 - mace: 22.6433 - val_loss: 0.7423 - val_mse: 0.3181 - val_mace: 23.7533

Epoch 00017: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 17/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7061 - mse: 0.2952 - mace: 22.5937  
Epoch 00017: val_loss did not improve from 0.73441
1016/1016 [==============================] - 110s 108ms/step - loss: 0.7061 - mse: 0.2953 - mace: 22.5943 - val_loss: 0.7377 - val_mse: 0.3148 - val_mace: 23.6049

Epoch 00018: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 18/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7052 - mse: 0.2947 - mace: 22.5657  
Epoch 00018: val_loss did not improve from 0.73441
1016/1016 [==============================] - 91s 90ms/step - loss: 0.7052 - mse: 0.2947 - mace: 22.5648 - val_loss: 0.7471 - val_mse: 0.3238 - val_mace: 23.9087

Epoch 00019: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 19/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7031 - mse: 0.2933 - mace: 22.4981  
Epoch 00019: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.7031 - mse: 0.2933 - mace: 22.4989 - val_loss: 0.7477 - val_mse: 0.3236 - val_mace: 23.9255

Epoch 00020: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 20/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.7031 - mse: 0.2936 - mace: 22.4997  
Epoch 00020: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.7031 - mse: 0.2935 - mace: 22.4984 - val_loss: 0.7432 - val_mse: 0.3186 - val_mace: 23.7814

Epoch 00021: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 21/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6978 - mse: 0.2902 - mace: 22.3311  
Epoch 00021: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 99ms/step - loss: 0.6979 - mse: 0.2902 - mace: 22.3319 - val_loss: 0.7384 - val_mse: 0.3148 - val_mace: 23.6275

Epoch 00022: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 22/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6983 - mse: 0.2908 - mace: 22.3447  
Epoch 00022: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.6983 - mse: 0.2908 - mace: 22.3443 - val_loss: 0.7380 - val_mse: 0.3142 - val_mace: 23.6153

Epoch 00023: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 23/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6991 - mse: 0.2910 - mace: 22.3707  
Epoch 00023: val_loss did not improve from 0.73441
1016/1016 [==============================] - 110s 108ms/step - loss: 0.6991 - mse: 0.2910 - mace: 22.3710 - val_loss: 0.7399 - val_mse: 0.3169 - val_mace: 23.6778

Epoch 00024: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 24/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6964 - mse: 0.2890 - mace: 22.2833 
Epoch 00024: val_loss did not improve from 0.73441
1016/1016 [==============================] - 92s 91ms/step - loss: 0.6963 - mse: 0.2890 - mace: 22.2831 - val_loss: 0.7457 - val_mse: 0.3218 - val_mace: 23.8609

Epoch 00025: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 25/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6955 - mse: 0.2885 - mace: 22.2546  
Epoch 00025: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 99ms/step - loss: 0.6955 - mse: 0.2886 - mace: 22.2553 - val_loss: 0.7432 - val_mse: 0.3181 - val_mace: 23.7838

Epoch 00026: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 26/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6980 - mse: 0.2900 - mace: 22.3358  
Epoch 00026: val_loss did not improve from 0.73441
1016/1016 [==============================] - 102s 100ms/step - loss: 0.6980 - mse: 0.2900 - mace: 22.3372 - val_loss: 0.7404 - val_mse: 0.3173 - val_mace: 23.6921

Epoch 00027: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 27/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6960 - mse: 0.2888 - mace: 22.2712  
Epoch 00027: val_loss did not improve from 0.73441
1016/1016 [==============================] - 109s 107ms/step - loss: 0.6960 - mse: 0.2888 - mace: 22.2715 - val_loss: 0.7381 - val_mse: 0.3146 - val_mace: 23.6199

Epoch 00028: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 28/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6960 - mse: 0.2886 - mace: 22.2732 
Epoch 00028: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6960 - mse: 0.2885 - mace: 22.2720 - val_loss: 0.7473 - val_mse: 0.3242 - val_mace: 23.9137

Epoch 00029: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 29/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6956 - mse: 0.2884 - mace: 22.2589  
Epoch 00029: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 99ms/step - loss: 0.6957 - mse: 0.2885 - mace: 22.2611 - val_loss: 0.7419 - val_mse: 0.3174 - val_mace: 23.7418

Epoch 00030: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 30/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6956 - mse: 0.2884 - mace: 22.2594  
Epoch 00030: val_loss did not improve from 0.73441
1016/1016 [==============================] - 91s 90ms/step - loss: 0.6956 - mse: 0.2884 - mace: 22.2595 - val_loss: 0.7364 - val_mse: 0.3137 - val_mace: 23.5643

Epoch 00031: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 31/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6912 - mse: 0.2858 - mace: 22.1191  
Epoch 00031: val_loss did not improve from 0.73441
1016/1016 [==============================] - 106s 104ms/step - loss: 0.6912 - mse: 0.2858 - mace: 22.1182 - val_loss: 0.7494 - val_mse: 0.3263 - val_mace: 23.9796

Epoch 00032: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 32/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6922 - mse: 0.2863 - mace: 22.1502 
Epoch 00032: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 96ms/step - loss: 0.6922 - mse: 0.2863 - mace: 22.1493 - val_loss: 0.7434 - val_mse: 0.3182 - val_mace: 23.7888

Epoch 00033: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 33/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6910 - mse: 0.2854 - mace: 22.1109  
Epoch 00033: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6910 - mse: 0.2854 - mace: 22.1110 - val_loss: 0.7474 - val_mse: 0.3243 - val_mace: 23.9159

Epoch 00034: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 34/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6906 - mse: 0.2856 - mace: 22.0997  
Epoch 00034: val_loss did not improve from 0.73441
1016/1016 [==============================] - 96s 94ms/step - loss: 0.6906 - mse: 0.2856 - mace: 22.0985 - val_loss: 0.7435 - val_mse: 0.3199 - val_mace: 23.7924

Epoch 00035: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 35/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6895 - mse: 0.2849 - mace: 22.0644  
Epoch 00035: val_loss did not improve from 0.73441
1016/1016 [==============================] - 102s 100ms/step - loss: 0.6895 - mse: 0.2849 - mace: 22.0648 - val_loss: 0.7475 - val_mse: 0.3259 - val_mace: 23.9194

Epoch 00036: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 36/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6897 - mse: 0.2848 - mace: 22.0698  
Epoch 00036: val_loss did not improve from 0.73441
1016/1016 [==============================] - 92s 91ms/step - loss: 0.6897 - mse: 0.2848 - mace: 22.0700 - val_loss: 0.7449 - val_mse: 0.3215 - val_mace: 23.8357

Epoch 00037: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 37/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6888 - mse: 0.2837 - mace: 22.0410  
Epoch 00037: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 99ms/step - loss: 0.6888 - mse: 0.2837 - mace: 22.0408 - val_loss: 0.7393 - val_mse: 0.3169 - val_mace: 23.6589

Epoch 00038: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 38/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6867 - mse: 0.2828 - mace: 21.9743  
Epoch 00038: val_loss did not improve from 0.73441
1016/1016 [==============================] - 106s 105ms/step - loss: 0.6867 - mse: 0.2827 - mace: 21.9731 - val_loss: 0.7382 - val_mse: 0.3172 - val_mace: 23.6208

Epoch 00039: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 39/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6864 - mse: 0.2824 - mace: 21.9664  
Epoch 00039: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 96ms/step - loss: 0.6864 - mse: 0.2824 - mace: 21.9660 - val_loss: 0.7363 - val_mse: 0.3137 - val_mace: 23.5607

Epoch 00040: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 40/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6845 - mse: 0.2812 - mace: 21.9026  
Epoch 00040: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 99ms/step - loss: 0.6844 - mse: 0.2812 - mace: 21.9010 - val_loss: 0.7416 - val_mse: 0.3186 - val_mace: 23.7299

Epoch 00041: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 41/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6805 - mse: 0.2788 - mace: 21.7760  
Epoch 00041: val_loss did not improve from 0.73441
1016/1016 [==============================] - 94s 93ms/step - loss: 0.6805 - mse: 0.2788 - mace: 21.7770 - val_loss: 0.7439 - val_mse: 0.3180 - val_mace: 23.8058

Epoch 00042: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 42/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6807 - mse: 0.2789 - mace: 21.7828  
Epoch 00042: val_loss did not improve from 0.73441
1016/1016 [==============================] - 94s 93ms/step - loss: 0.6807 - mse: 0.2789 - mace: 21.7834 - val_loss: 0.7446 - val_mse: 0.3215 - val_mace: 23.8272

Epoch 00043: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 43/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6786 - mse: 0.2774 - mace: 21.7167  
Epoch 00043: val_loss did not improve from 0.73441
1016/1016 [==============================] - 90s 89ms/step - loss: 0.6786 - mse: 0.2774 - mace: 21.7167 - val_loss: 0.7435 - val_mse: 0.3193 - val_mace: 23.7920

Epoch 00044: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 44/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6801 - mse: 0.2784 - mace: 21.7621  
Epoch 00044: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6801 - mse: 0.2784 - mace: 21.7627 - val_loss: 0.7407 - val_mse: 0.3182 - val_mace: 23.7013

Epoch 00045: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 45/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6797 - mse: 0.2782 - mace: 21.7491  
Epoch 00045: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 99ms/step - loss: 0.6797 - mse: 0.2782 - mace: 21.7505 - val_loss: 0.7446 - val_mse: 0.3224 - val_mace: 23.8272

Epoch 00046: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 46/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6791 - mse: 0.2775 - mace: 21.7315  
Epoch 00046: val_loss did not improve from 0.73441
1016/1016 [==============================] - 109s 107ms/step - loss: 0.6791 - mse: 0.2775 - mace: 21.7309 - val_loss: 0.7566 - val_mse: 0.3354 - val_mace: 24.2119

Epoch 00047: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 47/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6793 - mse: 0.2775 - mace: 21.7376  
Epoch 00047: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 99ms/step - loss: 0.6793 - mse: 0.2775 - mace: 21.7371 - val_loss: 0.7415 - val_mse: 0.3168 - val_mace: 23.7265

Epoch 00048: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 48/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6776 - mse: 0.2767 - mace: 21.6832  
Epoch 00048: val_loss did not improve from 0.73441
1016/1016 [==============================] - 92s 90ms/step - loss: 0.6776 - mse: 0.2767 - mace: 21.6832 - val_loss: 0.7607 - val_mse: 0.3400 - val_mace: 24.3431

Epoch 00049: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 49/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6794 - mse: 0.2781 - mace: 21.7423  
Epoch 00049: val_loss did not improve from 0.73441
1016/1016 [==============================] - 105s 103ms/step - loss: 0.6795 - mse: 0.2781 - mace: 21.7434 - val_loss: 0.7552 - val_mse: 0.3345 - val_mace: 24.1650

Epoch 00050: LearningRateScheduler reducing learning rate to 0.0004999999888241291.
Epoch 50/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6717 - mse: 0.2722 - mace: 21.4958  
Epoch 00050: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 100ms/step - loss: 0.6718 - mse: 0.2723 - mace: 21.4981 - val_loss: 0.7544 - val_mse: 0.3320 - val_mace: 24.1414

Epoch 00051: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 51/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6675 - mse: 0.2696 - mace: 21.3600  
Epoch 00051: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 100ms/step - loss: 0.6675 - mse: 0.2696 - mace: 21.3605 - val_loss: 0.7483 - val_mse: 0.3260 - val_mace: 23.9471

Epoch 00052: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 52/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6650 - mse: 0.2681 - mace: 21.2787  
Epoch 00052: val_loss did not improve from 0.73441
1016/1016 [==============================] - 102s 101ms/step - loss: 0.6650 - mse: 0.2681 - mace: 21.2797 - val_loss: 0.7470 - val_mse: 0.3258 - val_mace: 23.9038

Epoch 00053: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 53/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6651 - mse: 0.2678 - mace: 21.2821  
Epoch 00053: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 97ms/step - loss: 0.6651 - mse: 0.2678 - mace: 21.2818 - val_loss: 0.7479 - val_mse: 0.3242 - val_mace: 23.9332

Epoch 00054: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 54/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6642 - mse: 0.2674 - mace: 21.2533  
Epoch 00054: val_loss did not improve from 0.73441
1016/1016 [==============================] - 95s 93ms/step - loss: 0.6641 - mse: 0.2674 - mace: 21.2518 - val_loss: 0.7474 - val_mse: 0.3249 - val_mace: 23.9152

Epoch 00055: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 55/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6622 - mse: 0.2660 - mace: 21.1899 
Epoch 00055: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 99ms/step - loss: 0.6622 - mse: 0.2660 - mace: 21.1900 - val_loss: 0.7462 - val_mse: 0.3250 - val_mace: 23.8779

Epoch 00056: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 56/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6632 - mse: 0.2668 - mace: 21.2231  
Epoch 00056: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 97ms/step - loss: 0.6632 - mse: 0.2668 - mace: 21.2236 - val_loss: 0.7403 - val_mse: 0.3178 - val_mace: 23.6899

Epoch 00057: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 57/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6636 - mse: 0.2674 - mace: 21.2342  
Epoch 00057: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 96ms/step - loss: 0.6635 - mse: 0.2673 - mace: 21.2331 - val_loss: 0.7452 - val_mse: 0.3230 - val_mace: 23.8477

Epoch 00058: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 58/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6638 - mse: 0.2671 - mace: 21.2421  
Epoch 00058: val_loss did not improve from 0.73441
1016/1016 [==============================] - 107s 105ms/step - loss: 0.6638 - mse: 0.2670 - mace: 21.2419 - val_loss: 0.7399 - val_mse: 0.3173 - val_mace: 23.6753

Epoch 00059: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 59/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6633 - mse: 0.2668 - mace: 21.2259 
Epoch 00059: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 99ms/step - loss: 0.6633 - mse: 0.2668 - mace: 21.2262 - val_loss: 0.7455 - val_mse: 0.3235 - val_mace: 23.8556

Epoch 00060: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 60/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6640 - mse: 0.2669 - mace: 21.2484  
Epoch 00060: val_loss did not improve from 0.73441
1016/1016 [==============================] - 92s 91ms/step - loss: 0.6640 - mse: 0.2669 - mace: 21.2496 - val_loss: 0.7501 - val_mse: 0.3275 - val_mace: 24.0041

Epoch 00061: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 61/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6618 - mse: 0.2657 - mace: 21.1786  
Epoch 00061: val_loss did not improve from 0.73441
1016/1016 [==============================] - 95s 93ms/step - loss: 0.6618 - mse: 0.2657 - mace: 21.1781 - val_loss: 0.7486 - val_mse: 0.3261 - val_mace: 23.9563

Epoch 00062: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 62/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6619 - mse: 0.2663 - mace: 21.1798  
Epoch 00062: val_loss did not improve from 0.73441
1016/1016 [==============================] - 106s 104ms/step - loss: 0.6619 - mse: 0.2663 - mace: 21.1821 - val_loss: 0.7447 - val_mse: 0.3225 - val_mace: 23.8314

Epoch 00063: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 63/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6604 - mse: 0.2649 - mace: 21.1336 
Epoch 00063: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 97ms/step - loss: 0.6604 - mse: 0.2649 - mace: 21.1335 - val_loss: 0.7420 - val_mse: 0.3200 - val_mace: 23.7430

Epoch 00064: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 64/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6624 - mse: 0.2661 - mace: 21.1977  
Epoch 00064: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.6625 - mse: 0.2662 - mace: 21.1985 - val_loss: 0.7482 - val_mse: 0.3272 - val_mace: 23.9425

Epoch 00065: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 65/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6596 - mse: 0.2642 - mace: 21.1064  
Epoch 00065: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 97ms/step - loss: 0.6596 - mse: 0.2642 - mace: 21.1059 - val_loss: 0.7471 - val_mse: 0.3261 - val_mace: 23.9071

Epoch 00066: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 66/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6606 - mse: 0.2652 - mace: 21.1399  
Epoch 00066: val_loss did not improve from 0.73441
1016/1016 [==============================] - 96s 95ms/step - loss: 0.6606 - mse: 0.2652 - mace: 21.1397 - val_loss: 0.7464 - val_mse: 0.3232 - val_mace: 23.8850

Epoch 00067: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 67/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6611 - mse: 0.2650 - mace: 21.1560  
Epoch 00067: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 96ms/step - loss: 0.6611 - mse: 0.2650 - mace: 21.1549 - val_loss: 0.7427 - val_mse: 0.3203 - val_mace: 23.7649

Epoch 00068: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 68/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6607 - mse: 0.2647 - mace: 21.1432  
Epoch 00068: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 96ms/step - loss: 0.6608 - mse: 0.2647 - mace: 21.1442 - val_loss: 0.7407 - val_mse: 0.3190 - val_mace: 23.7021

Epoch 00069: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 69/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6591 - mse: 0.2642 - mace: 21.0919  
Epoch 00069: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 100ms/step - loss: 0.6591 - mse: 0.2642 - mace: 21.0923 - val_loss: 0.7451 - val_mse: 0.3239 - val_mace: 23.8445

Epoch 00070: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 70/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6596 - mse: 0.2644 - mace: 21.1057  
Epoch 00070: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 99ms/step - loss: 0.6595 - mse: 0.2643 - mace: 21.1051 - val_loss: 0.7469 - val_mse: 0.3252 - val_mace: 23.9018

Epoch 00071: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 71/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6568 - mse: 0.2627 - mace: 21.0184  
Epoch 00071: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 95ms/step - loss: 0.6568 - mse: 0.2626 - mace: 21.0173 - val_loss: 0.7437 - val_mse: 0.3225 - val_mace: 23.7996

Epoch 00072: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 72/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6582 - mse: 0.2633 - mace: 21.0613  
Epoch 00072: val_loss did not improve from 0.73441
1016/1016 [==============================] - 92s 91ms/step - loss: 0.6582 - mse: 0.2633 - mace: 21.0613 - val_loss: 0.7466 - val_mse: 0.3245 - val_mace: 23.8908

Epoch 00073: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 73/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6571 - mse: 0.2631 - mace: 21.0287  
Epoch 00073: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.6572 - mse: 0.2632 - mace: 21.0299 - val_loss: 0.7423 - val_mse: 0.3206 - val_mace: 23.7526

Epoch 00074: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 74/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6585 - mse: 0.2637 - mace: 21.0716  
Epoch 00074: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.6585 - mse: 0.2637 - mace: 21.0713 - val_loss: 0.7449 - val_mse: 0.3224 - val_mace: 23.8363

Epoch 00075: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 75/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6575 - mse: 0.2632 - mace: 21.0391  
Epoch 00075: val_loss did not improve from 0.73441
1016/1016 [==============================] - 108s 107ms/step - loss: 0.6575 - mse: 0.2632 - mace: 21.0388 - val_loss: 0.7461 - val_mse: 0.3251 - val_mace: 23.8756

Epoch 00076: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 76/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6577 - mse: 0.2628 - mace: 21.0450 
Epoch 00076: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 95ms/step - loss: 0.6577 - mse: 0.2628 - mace: 21.0458 - val_loss: 0.7450 - val_mse: 0.3228 - val_mace: 23.8389

Epoch 00077: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 77/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6576 - mse: 0.2629 - mace: 21.0423  
Epoch 00077: val_loss did not improve from 0.73441
1016/1016 [==============================] - 103s 101ms/step - loss: 0.6576 - mse: 0.2629 - mace: 21.0440 - val_loss: 0.7405 - val_mse: 0.3187 - val_mace: 23.6966

Epoch 00078: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 78/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6586 - mse: 0.2636 - mace: 21.0767  
Epoch 00078: val_loss did not improve from 0.73441
1016/1016 [==============================] - 90s 89ms/step - loss: 0.6586 - mse: 0.2636 - mace: 21.0764 - val_loss: 0.7458 - val_mse: 0.3231 - val_mace: 23.8656

Epoch 00079: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 79/196
1014/1016 [============================>.] - ETA: 0s - loss: 0.6583 - mse: 0.2635 - mace: 21.0644  
Epoch 00079: val_loss did not improve from 0.73441
1016/1016 [==============================] - 107s 105ms/step - loss: 0.6582 - mse: 0.2635 - mace: 21.0630 - val_loss: 0.7407 - val_mse: 0.3190 - val_mace: 23.7009

Epoch 00080: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 80/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6575 - mse: 0.2627 - mace: 21.0401  
Epoch 00080: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 100ms/step - loss: 0.6575 - mse: 0.2627 - mace: 21.0392 - val_loss: 0.7411 - val_mse: 0.3198 - val_mace: 23.7139

Epoch 00081: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 81/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6565 - mse: 0.2625 - mace: 21.0069  
Epoch 00081: val_loss did not improve from 0.73441
1016/1016 [==============================] - 104s 102ms/step - loss: 0.6565 - mse: 0.2625 - mace: 21.0071 - val_loss: 0.7434 - val_mse: 0.3208 - val_mace: 23.7875

Epoch 00082: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 82/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6573 - mse: 0.2630 - mace: 21.0321 
Epoch 00082: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 95ms/step - loss: 0.6573 - mse: 0.2630 - mace: 21.0334 - val_loss: 0.7412 - val_mse: 0.3196 - val_mace: 23.7199

Epoch 00083: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 83/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6563 - mse: 0.2623 - mace: 21.0016  
Epoch 00083: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 95ms/step - loss: 0.6563 - mse: 0.2623 - mace: 21.0013 - val_loss: 0.7410 - val_mse: 0.3195 - val_mace: 23.7122

Epoch 00084: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 84/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6560 - mse: 0.2620 - mace: 20.9936  
Epoch 00084: val_loss did not improve from 0.73441
1016/1016 [==============================] - 89s 87ms/step - loss: 0.6561 - mse: 0.2620 - mace: 20.9951 - val_loss: 0.7422 - val_mse: 0.3203 - val_mace: 23.7490

Epoch 00085: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 85/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6575 - mse: 0.2631 - mace: 21.0409  
Epoch 00085: val_loss did not improve from 0.73441
1016/1016 [==============================] - 108s 107ms/step - loss: 0.6576 - mse: 0.2631 - mace: 21.0418 - val_loss: 0.7456 - val_mse: 0.3237 - val_mace: 23.8587

Epoch 00086: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 86/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6569 - mse: 0.2620 - mace: 21.0193 
Epoch 00086: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 96ms/step - loss: 0.6568 - mse: 0.2620 - mace: 21.0191 - val_loss: 0.7441 - val_mse: 0.3225 - val_mace: 23.8110

Epoch 00087: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 87/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6562 - mse: 0.2624 - mace: 20.9973  
Epoch 00087: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.6562 - mse: 0.2624 - mace: 20.9980 - val_loss: 0.7450 - val_mse: 0.3234 - val_mace: 23.8394

Epoch 00088: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 88/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6575 - mse: 0.2629 - mace: 21.0384  
Epoch 00088: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.6574 - mse: 0.2629 - mace: 21.0365 - val_loss: 0.7434 - val_mse: 0.3218 - val_mace: 23.7891

Epoch 00089: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 89/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6552 - mse: 0.2613 - mace: 20.9657  
Epoch 00089: val_loss did not improve from 0.73441
1016/1016 [==============================] - 106s 104ms/step - loss: 0.6552 - mse: 0.2613 - mace: 20.9659 - val_loss: 0.7402 - val_mse: 0.3184 - val_mace: 23.6861

Epoch 00090: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 90/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6567 - mse: 0.2622 - mace: 21.0156 
Epoch 00090: val_loss did not improve from 0.73441
1016/1016 [==============================] - 91s 89ms/step - loss: 0.6568 - mse: 0.2623 - mace: 21.0163 - val_loss: 0.7436 - val_mse: 0.3217 - val_mace: 23.7965

Epoch 00091: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 91/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6549 - mse: 0.2616 - mace: 20.9556  
Epoch 00091: val_loss did not improve from 0.73441
1016/1016 [==============================] - 94s 93ms/step - loss: 0.6549 - mse: 0.2616 - mace: 20.9570 - val_loss: 0.7468 - val_mse: 0.3250 - val_mace: 23.8985

Epoch 00092: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 92/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6557 - mse: 0.2619 - mace: 20.9813  
Epoch 00092: val_loss did not improve from 0.73441
1016/1016 [==============================] - 95s 94ms/step - loss: 0.6557 - mse: 0.2619 - mace: 20.9810 - val_loss: 0.7420 - val_mse: 0.3199 - val_mace: 23.7442

Epoch 00093: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 93/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6540 - mse: 0.2609 - mace: 20.9295  
Epoch 00093: val_loss did not improve from 0.73441
1016/1016 [==============================] - 103s 101ms/step - loss: 0.6541 - mse: 0.2610 - mace: 20.9301 - val_loss: 0.7446 - val_mse: 0.3233 - val_mace: 23.8278

Epoch 00094: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 94/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6542 - mse: 0.2607 - mace: 20.9355  
Epoch 00094: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6543 - mse: 0.2607 - mace: 20.9362 - val_loss: 0.7429 - val_mse: 0.3217 - val_mace: 23.7715

Epoch 00095: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 95/196
1014/1016 [============================>.] - ETA: 0s - loss: 0.6549 - mse: 0.2611 - mace: 20.9572  
Epoch 00095: val_loss did not improve from 0.73441
1016/1016 [==============================] - 107s 106ms/step - loss: 0.6549 - mse: 0.2611 - mace: 20.9554 - val_loss: 0.7488 - val_mse: 0.3277 - val_mace: 23.9624

Epoch 00096: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 96/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6538 - mse: 0.2605 - mace: 20.9202 
Epoch 00096: val_loss did not improve from 0.73441
1016/1016 [==============================] - 91s 90ms/step - loss: 0.6537 - mse: 0.2605 - mace: 20.9189 - val_loss: 0.7435 - val_mse: 0.3214 - val_mace: 23.7929

Epoch 00097: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 97/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6549 - mse: 0.2614 - mace: 20.9556  
Epoch 00097: val_loss did not improve from 0.73441
1016/1016 [==============================] - 96s 95ms/step - loss: 0.6549 - mse: 0.2614 - mace: 20.9555 - val_loss: 0.7413 - val_mse: 0.3195 - val_mace: 23.7201

Epoch 00098: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 98/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6524 - mse: 0.2596 - mace: 20.8772  
Epoch 00098: val_loss did not improve from 0.73441
1016/1016 [==============================] - 103s 101ms/step - loss: 0.6524 - mse: 0.2596 - mace: 20.8770 - val_loss: 0.7464 - val_mse: 0.3253 - val_mace: 23.8860

Epoch 00099: LearningRateScheduler reducing learning rate to 4.9999996554106475e-05.
Epoch 99/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6546 - mse: 0.2613 - mace: 20.9465  
Epoch 00099: val_loss did not improve from 0.73441
1016/1016 [==============================] - 105s 104ms/step - loss: 0.6545 - mse: 0.2612 - mace: 20.9445 - val_loss: 0.7455 - val_mse: 0.3241 - val_mace: 23.8558

Epoch 00100: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 100/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6555 - mse: 0.2617 - mace: 20.9774 
Epoch 00100: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 96ms/step - loss: 0.6556 - mse: 0.2617 - mace: 20.9787 - val_loss: 0.7437 - val_mse: 0.3226 - val_mace: 23.7981

Epoch 00101: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 101/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6528 - mse: 0.2599 - mace: 20.8908  
Epoch 00101: val_loss did not improve from 0.73441
1016/1016 [==============================] - 88s 86ms/step - loss: 0.6528 - mse: 0.2599 - mace: 20.8893 - val_loss: 0.7438 - val_mse: 0.3219 - val_mace: 23.8005

Epoch 00102: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 102/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6522 - mse: 0.2593 - mace: 20.8700  
Epoch 00102: val_loss did not improve from 0.73441
1016/1016 [==============================] - 91s 90ms/step - loss: 0.6522 - mse: 0.2593 - mace: 20.8713 - val_loss: 0.7463 - val_mse: 0.3242 - val_mace: 23.8825

Epoch 00103: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 103/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6520 - mse: 0.2593 - mace: 20.8642  
Epoch 00103: val_loss did not improve from 0.73441
1016/1016 [==============================] - 108s 106ms/step - loss: 0.6520 - mse: 0.2593 - mace: 20.8636 - val_loss: 0.7421 - val_mse: 0.3212 - val_mace: 23.7482

Epoch 00104: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 104/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6531 - mse: 0.2600 - mace: 20.8981 
Epoch 00104: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 97ms/step - loss: 0.6530 - mse: 0.2600 - mace: 20.8974 - val_loss: 0.7471 - val_mse: 0.3245 - val_mace: 23.9073

Epoch 00105: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 105/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6538 - mse: 0.2602 - mace: 20.9202  
Epoch 00105: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.6538 - mse: 0.2602 - mace: 20.9204 - val_loss: 0.7427 - val_mse: 0.3212 - val_mace: 23.7648

Epoch 00106: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 106/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6526 - mse: 0.2599 - mace: 20.8831  
Epoch 00106: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 97ms/step - loss: 0.6526 - mse: 0.2599 - mace: 20.8817 - val_loss: 0.7421 - val_mse: 0.3208 - val_mace: 23.7465

Epoch 00107: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 107/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6525 - mse: 0.2599 - mace: 20.8789  
Epoch 00107: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 100ms/step - loss: 0.6525 - mse: 0.2599 - mace: 20.8793 - val_loss: 0.7440 - val_mse: 0.3229 - val_mace: 23.8094

Epoch 00108: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 108/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6525 - mse: 0.2595 - mace: 20.8801  
Epoch 00108: val_loss did not improve from 0.73441
1016/1016 [==============================] - 91s 89ms/step - loss: 0.6525 - mse: 0.2595 - mace: 20.8802 - val_loss: 0.7427 - val_mse: 0.3212 - val_mace: 23.7673

Epoch 00109: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 109/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6515 - mse: 0.2588 - mace: 20.8493  
Epoch 00109: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 97ms/step - loss: 0.6516 - mse: 0.2588 - mace: 20.8496 - val_loss: 0.7442 - val_mse: 0.3222 - val_mace: 23.8153

Epoch 00110: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 110/196
1014/1016 [============================>.] - ETA: 0s - loss: 0.6529 - mse: 0.2600 - mace: 20.8942  
Epoch 00110: val_loss did not improve from 0.73441
1016/1016 [==============================] - 107s 106ms/step - loss: 0.6529 - mse: 0.2600 - mace: 20.8934 - val_loss: 0.7441 - val_mse: 0.3229 - val_mace: 23.8123

Epoch 00111: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 111/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6519 - mse: 0.2592 - mace: 20.8599 
Epoch 00111: val_loss did not improve from 0.73441
1016/1016 [==============================] - 95s 94ms/step - loss: 0.6519 - mse: 0.2592 - mace: 20.8606 - val_loss: 0.7455 - val_mse: 0.3241 - val_mace: 23.8546

Epoch 00112: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 112/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6537 - mse: 0.2607 - mace: 20.9193  
Epoch 00112: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 95ms/step - loss: 0.6538 - mse: 0.2608 - mace: 20.9200 - val_loss: 0.7478 - val_mse: 0.3253 - val_mace: 23.9289

Epoch 00113: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 113/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6519 - mse: 0.2592 - mace: 20.8609  
Epoch 00113: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6519 - mse: 0.2593 - mace: 20.8612 - val_loss: 0.7433 - val_mse: 0.3219 - val_mace: 23.7851

Epoch 00114: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 114/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6521 - mse: 0.2595 - mace: 20.8664  
Epoch 00114: val_loss did not improve from 0.73441
1016/1016 [==============================] - 96s 95ms/step - loss: 0.6520 - mse: 0.2595 - mace: 20.8654 - val_loss: 0.7427 - val_mse: 0.3215 - val_mace: 23.7672

Epoch 00115: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 115/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6521 - mse: 0.2597 - mace: 20.8680  
Epoch 00115: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 95ms/step - loss: 0.6521 - mse: 0.2597 - mace: 20.8685 - val_loss: 0.7424 - val_mse: 0.3212 - val_mace: 23.7555

Epoch 00116: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 116/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6538 - mse: 0.2605 - mace: 20.9224  
Epoch 00116: val_loss did not improve from 0.73441
1016/1016 [==============================] - 102s 100ms/step - loss: 0.6538 - mse: 0.2605 - mace: 20.9224 - val_loss: 0.7423 - val_mse: 0.3209 - val_mace: 23.7550

Epoch 00117: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 117/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6522 - mse: 0.2591 - mace: 20.8688  
Epoch 00117: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 96ms/step - loss: 0.6522 - mse: 0.2591 - mace: 20.8694 - val_loss: 0.7441 - val_mse: 0.3221 - val_mace: 23.8104

Epoch 00118: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 118/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6513 - mse: 0.2589 - mace: 20.8415  
Epoch 00118: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 96ms/step - loss: 0.6513 - mse: 0.2589 - mace: 20.8415 - val_loss: 0.7434 - val_mse: 0.3218 - val_mace: 23.7877

Epoch 00119: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 119/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6517 - mse: 0.2590 - mace: 20.8538  
Epoch 00119: val_loss did not improve from 0.73441
1016/1016 [==============================] - 93s 91ms/step - loss: 0.6517 - mse: 0.2590 - mace: 20.8537 - val_loss: 0.7419 - val_mse: 0.3206 - val_mace: 23.7403

Epoch 00120: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 120/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6524 - mse: 0.2597 - mace: 20.8770  
Epoch 00120: val_loss did not improve from 0.73441
1016/1016 [==============================] - 108s 106ms/step - loss: 0.6524 - mse: 0.2596 - mace: 20.8766 - val_loss: 0.7482 - val_mse: 0.3263 - val_mace: 23.9434

Epoch 00121: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 121/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6524 - mse: 0.2595 - mace: 20.8752  
Epoch 00121: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6523 - mse: 0.2594 - mace: 20.8731 - val_loss: 0.7438 - val_mse: 0.3223 - val_mace: 23.8004

Epoch 00122: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 122/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6543 - mse: 0.2607 - mace: 20.9381  
Epoch 00122: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 97ms/step - loss: 0.6544 - mse: 0.2608 - mace: 20.9393 - val_loss: 0.7416 - val_mse: 0.3208 - val_mace: 23.7315

Epoch 00123: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 123/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6518 - mse: 0.2592 - mace: 20.8565  
Epoch 00123: val_loss did not improve from 0.73441
1016/1016 [==============================] - 96s 95ms/step - loss: 0.6517 - mse: 0.2592 - mace: 20.8552 - val_loss: 0.7471 - val_mse: 0.3250 - val_mace: 23.9086

Epoch 00124: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 124/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6520 - mse: 0.2592 - mace: 20.8632  
Epoch 00124: val_loss did not improve from 0.73441
1016/1016 [==============================] - 108s 106ms/step - loss: 0.6520 - mse: 0.2592 - mace: 20.8641 - val_loss: 0.7439 - val_mse: 0.3225 - val_mace: 23.8034

Epoch 00125: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 125/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6517 - mse: 0.2589 - mace: 20.8530  
Epoch 00125: val_loss did not improve from 0.73441
1016/1016 [==============================] - 92s 91ms/step - loss: 0.6517 - mse: 0.2589 - mace: 20.8541 - val_loss: 0.7408 - val_mse: 0.3192 - val_mace: 23.7047

Epoch 00126: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 126/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6513 - mse: 0.2589 - mace: 20.8427  
Epoch 00126: val_loss did not improve from 0.73441
1016/1016 [==============================] - 103s 101ms/step - loss: 0.6513 - mse: 0.2590 - mace: 20.8431 - val_loss: 0.7409 - val_mse: 0.3195 - val_mace: 23.7081

Epoch 00127: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 127/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6517 - mse: 0.2590 - mace: 20.8546 
Epoch 00127: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 97ms/step - loss: 0.6517 - mse: 0.2590 - mace: 20.8552 - val_loss: 0.7419 - val_mse: 0.3208 - val_mace: 23.7396

Epoch 00128: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 128/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6514 - mse: 0.2590 - mace: 20.8457  
Epoch 00128: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 97ms/step - loss: 0.6514 - mse: 0.2590 - mace: 20.8460 - val_loss: 0.7454 - val_mse: 0.3233 - val_mace: 23.8532

Epoch 00129: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 129/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6520 - mse: 0.2594 - mace: 20.8635  
Epoch 00129: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 96ms/step - loss: 0.6520 - mse: 0.2594 - mace: 20.8628 - val_loss: 0.7428 - val_mse: 0.3219 - val_mace: 23.7711

Epoch 00130: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 130/196
1014/1016 [============================>.] - ETA: 0s - loss: 0.6538 - mse: 0.2607 - mace: 20.9228  
Epoch 00130: val_loss did not improve from 0.73441
1016/1016 [==============================] - 108s 107ms/step - loss: 0.6539 - mse: 0.2607 - mace: 20.9238 - val_loss: 0.7429 - val_mse: 0.3215 - val_mace: 23.7712

Epoch 00131: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 131/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6508 - mse: 0.2585 - mace: 20.8254 
Epoch 00131: val_loss did not improve from 0.73441
1016/1016 [==============================] - 88s 87ms/step - loss: 0.6508 - mse: 0.2585 - mace: 20.8253 - val_loss: 0.7467 - val_mse: 0.3251 - val_mace: 23.8948

Epoch 00132: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 132/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6517 - mse: 0.2592 - mace: 20.8549  
Epoch 00132: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 97ms/step - loss: 0.6517 - mse: 0.2591 - mace: 20.8544 - val_loss: 0.7419 - val_mse: 0.3212 - val_mace: 23.7422

Epoch 00133: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 133/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6503 - mse: 0.2580 - mace: 20.8110  
Epoch 00133: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 97ms/step - loss: 0.6503 - mse: 0.2580 - mace: 20.8111 - val_loss: 0.7431 - val_mse: 0.3216 - val_mace: 23.7798

Epoch 00134: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 134/196
1014/1016 [============================>.] - ETA: 0s - loss: 0.6531 - mse: 0.2603 - mace: 20.8987  
Epoch 00134: val_loss did not improve from 0.73441
1016/1016 [==============================] - 108s 107ms/step - loss: 0.6531 - mse: 0.2603 - mace: 20.8984 - val_loss: 0.7424 - val_mse: 0.3214 - val_mace: 23.7578

Epoch 00135: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 135/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6501 - mse: 0.2580 - mace: 20.8034 
Epoch 00135: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.6501 - mse: 0.2580 - mace: 20.8038 - val_loss: 0.7459 - val_mse: 0.3242 - val_mace: 23.8701

Epoch 00136: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 136/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6528 - mse: 0.2597 - mace: 20.8894  
Epoch 00136: val_loss did not improve from 0.73441
1016/1016 [==============================] - 95s 94ms/step - loss: 0.6528 - mse: 0.2597 - mace: 20.8889 - val_loss: 0.7397 - val_mse: 0.3189 - val_mace: 23.6692

Epoch 00137: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 137/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6513 - mse: 0.2589 - mace: 20.8403  
Epoch 00137: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 96ms/step - loss: 0.6512 - mse: 0.2589 - mace: 20.8390 - val_loss: 0.7454 - val_mse: 0.3234 - val_mace: 23.8536

Epoch 00138: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 138/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6512 - mse: 0.2589 - mace: 20.8369  
Epoch 00138: val_loss did not improve from 0.73441
1016/1016 [==============================] - 91s 89ms/step - loss: 0.6511 - mse: 0.2589 - mace: 20.8360 - val_loss: 0.7446 - val_mse: 0.3227 - val_mace: 23.8283

Epoch 00139: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 139/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6535 - mse: 0.2601 - mace: 20.9109  
Epoch 00139: val_loss did not improve from 0.73441
1016/1016 [==============================] - 102s 101ms/step - loss: 0.6534 - mse: 0.2600 - mace: 20.9096 - val_loss: 0.7399 - val_mse: 0.3189 - val_mace: 23.6780

Epoch 00140: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 140/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6514 - mse: 0.2594 - mace: 20.8447 
Epoch 00140: val_loss did not improve from 0.73441
1016/1016 [==============================] - 94s 93ms/step - loss: 0.6514 - mse: 0.2594 - mace: 20.8442 - val_loss: 0.7417 - val_mse: 0.3205 - val_mace: 23.7338

Epoch 00141: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 141/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6495 - mse: 0.2577 - mace: 20.7854  
Epoch 00141: val_loss did not improve from 0.73441
1016/1016 [==============================] - 104s 103ms/step - loss: 0.6495 - mse: 0.2576 - mace: 20.7844 - val_loss: 0.7406 - val_mse: 0.3195 - val_mace: 23.6991

Epoch 00142: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 142/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6526 - mse: 0.2599 - mace: 20.8819  
Epoch 00142: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 99ms/step - loss: 0.6526 - mse: 0.2599 - mace: 20.8824 - val_loss: 0.7435 - val_mse: 0.3222 - val_mace: 23.7916

Epoch 00143: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 143/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6516 - mse: 0.2592 - mace: 20.8523  
Epoch 00143: val_loss did not improve from 0.73441
1016/1016 [==============================] - 90s 89ms/step - loss: 0.6516 - mse: 0.2592 - mace: 20.8525 - val_loss: 0.7458 - val_mse: 0.3240 - val_mace: 23.8664

Epoch 00144: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 144/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6533 - mse: 0.2602 - mace: 20.9067  
Epoch 00144: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 95ms/step - loss: 0.6533 - mse: 0.2602 - mace: 20.9067 - val_loss: 0.7406 - val_mse: 0.3195 - val_mace: 23.6993

Epoch 00145: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 145/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6514 - mse: 0.2590 - mace: 20.8457  
Epoch 00145: val_loss did not improve from 0.73441
1016/1016 [==============================] - 107s 105ms/step - loss: 0.6514 - mse: 0.2590 - mace: 20.8461 - val_loss: 0.7418 - val_mse: 0.3206 - val_mace: 23.7381

Epoch 00146: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 146/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6533 - mse: 0.2601 - mace: 20.9071  
Epoch 00146: val_loss did not improve from 0.73441
1016/1016 [==============================] - 94s 93ms/step - loss: 0.6533 - mse: 0.2600 - mace: 20.9056 - val_loss: 0.7414 - val_mse: 0.3199 - val_mace: 23.7244

Epoch 00147: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 147/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6505 - mse: 0.2584 - mace: 20.8173  
Epoch 00147: val_loss did not improve from 0.73441
1016/1016 [==============================] - 107s 106ms/step - loss: 0.6505 - mse: 0.2584 - mace: 20.8167 - val_loss: 0.7433 - val_mse: 0.3222 - val_mace: 23.7855

Epoch 00148: LearningRateScheduler reducing learning rate to 4.999999509891496e-06.
Epoch 148/196
1014/1016 [============================>.] - ETA: 0s - loss: 0.6511 - mse: 0.2589 - mace: 20.8339  
Epoch 00148: val_loss did not improve from 0.73441
1016/1016 [==============================] - 95s 93ms/step - loss: 0.6510 - mse: 0.2589 - mace: 20.8330 - val_loss: 0.7464 - val_mse: 0.3242 - val_mace: 23.8834

Epoch 00149: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 149/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6516 - mse: 0.2588 - mace: 20.8516  
Epoch 00149: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 96ms/step - loss: 0.6516 - mse: 0.2588 - mace: 20.8509 - val_loss: 0.7426 - val_mse: 0.3215 - val_mace: 23.7624

Epoch 00150: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 150/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6507 - mse: 0.2585 - mace: 20.8210  
Epoch 00150: val_loss did not improve from 0.73441
1016/1016 [==============================] - 91s 90ms/step - loss: 0.6506 - mse: 0.2584 - mace: 20.8188 - val_loss: 0.7434 - val_mse: 0.3215 - val_mace: 23.7885

Epoch 00151: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 151/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6497 - mse: 0.2578 - mace: 20.7915  
Epoch 00151: val_loss did not improve from 0.73441
1016/1016 [==============================] - 103s 101ms/step - loss: 0.6498 - mse: 0.2578 - mace: 20.7923 - val_loss: 0.7455 - val_mse: 0.3236 - val_mace: 23.8565

Epoch 00152: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 152/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6515 - mse: 0.2589 - mace: 20.8470 
Epoch 00152: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 96ms/step - loss: 0.6515 - mse: 0.2589 - mace: 20.8475 - val_loss: 0.7416 - val_mse: 0.3206 - val_mace: 23.7300

Epoch 00153: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 153/196
1014/1016 [============================>.] - ETA: 0s - loss: 0.6535 - mse: 0.2602 - mace: 20.9131  
Epoch 00153: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 96ms/step - loss: 0.6536 - mse: 0.2603 - mace: 20.9137 - val_loss: 0.7428 - val_mse: 0.3217 - val_mace: 23.7703

Epoch 00154: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 154/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6514 - mse: 0.2591 - mace: 20.8447  
Epoch 00154: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 96ms/step - loss: 0.6513 - mse: 0.2590 - mace: 20.8425 - val_loss: 0.7406 - val_mse: 0.3193 - val_mace: 23.6978

Epoch 00155: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 155/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6508 - mse: 0.2586 - mace: 20.8265  
Epoch 00155: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 97ms/step - loss: 0.6508 - mse: 0.2586 - mace: 20.8265 - val_loss: 0.7422 - val_mse: 0.3206 - val_mace: 23.7514

Epoch 00156: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 156/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6518 - mse: 0.2594 - mace: 20.8576 
Epoch 00156: val_loss did not improve from 0.73441
1016/1016 [==============================] - 88s 87ms/step - loss: 0.6518 - mse: 0.2594 - mace: 20.8563 - val_loss: 0.7451 - val_mse: 0.3229 - val_mace: 23.8438

Epoch 00157: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 157/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6518 - mse: 0.2592 - mace: 20.8587 
Epoch 00157: val_loss did not improve from 0.73441
1016/1016 [==============================] - 103s 102ms/step - loss: 0.6518 - mse: 0.2592 - mace: 20.8567 - val_loss: 0.7412 - val_mse: 0.3200 - val_mace: 23.7199

Epoch 00158: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 158/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6516 - mse: 0.2591 - mace: 20.8524  
Epoch 00158: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6517 - mse: 0.2591 - mace: 20.8532 - val_loss: 0.7422 - val_mse: 0.3212 - val_mace: 23.7510

Epoch 00159: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 159/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6506 - mse: 0.2582 - mace: 20.8188  
Epoch 00159: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 97ms/step - loss: 0.6506 - mse: 0.2582 - mace: 20.8185 - val_loss: 0.7418 - val_mse: 0.3203 - val_mace: 23.7373

Epoch 00160: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 160/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6516 - mse: 0.2589 - mace: 20.8501  
Epoch 00160: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 99ms/step - loss: 0.6515 - mse: 0.2588 - mace: 20.8492 - val_loss: 0.7478 - val_mse: 0.3257 - val_mace: 23.9295

Epoch 00161: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 161/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6515 - mse: 0.2588 - mace: 20.8472  
Epoch 00161: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6514 - mse: 0.2588 - mace: 20.8459 - val_loss: 0.7418 - val_mse: 0.3210 - val_mace: 23.7379

Epoch 00162: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 162/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6513 - mse: 0.2588 - mace: 20.8407  
Epoch 00162: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 97ms/step - loss: 0.6514 - mse: 0.2589 - mace: 20.8437 - val_loss: 0.7444 - val_mse: 0.3222 - val_mace: 23.8208

Epoch 00163: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 163/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6520 - mse: 0.2593 - mace: 20.8624  
Epoch 00163: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.6519 - mse: 0.2593 - mace: 20.8614 - val_loss: 0.7422 - val_mse: 0.3210 - val_mace: 23.7501

Epoch 00164: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 164/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6499 - mse: 0.2578 - mace: 20.7979  
Epoch 00164: val_loss did not improve from 0.73441
1016/1016 [==============================] - 95s 93ms/step - loss: 0.6500 - mse: 0.2578 - mace: 20.7989 - val_loss: 0.7450 - val_mse: 0.3230 - val_mace: 23.8409

Epoch 00165: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 165/196
1014/1016 [============================>.] - ETA: 0s - loss: 0.6508 - mse: 0.2584 - mace: 20.8246  
Epoch 00165: val_loss did not improve from 0.73441
1016/1016 [==============================] - 105s 103ms/step - loss: 0.6507 - mse: 0.2583 - mace: 20.8236 - val_loss: 0.7414 - val_mse: 0.3205 - val_mace: 23.7238

Epoch 00166: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 166/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6519 - mse: 0.2594 - mace: 20.8601 
Epoch 00166: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6518 - mse: 0.2594 - mace: 20.8592 - val_loss: 0.7412 - val_mse: 0.3197 - val_mace: 23.7176

Epoch 00167: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 167/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6518 - mse: 0.2595 - mace: 20.8569  
Epoch 00167: val_loss did not improve from 0.73441
1016/1016 [==============================] - 92s 90ms/step - loss: 0.6518 - mse: 0.2595 - mace: 20.8565 - val_loss: 0.7421 - val_mse: 0.3207 - val_mace: 23.7458

Epoch 00168: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 168/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6518 - mse: 0.2594 - mace: 20.8583  
Epoch 00168: val_loss did not improve from 0.73441
1016/1016 [==============================] - 102s 100ms/step - loss: 0.6518 - mse: 0.2594 - mace: 20.8580 - val_loss: 0.7409 - val_mse: 0.3198 - val_mace: 23.7072

Epoch 00169: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 169/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6505 - mse: 0.2581 - mace: 20.8150  
Epoch 00169: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 97ms/step - loss: 0.6504 - mse: 0.2581 - mace: 20.8141 - val_loss: 0.7418 - val_mse: 0.3206 - val_mace: 23.7392

Epoch 00170: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 170/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6510 - mse: 0.2586 - mace: 20.8314  
Epoch 00170: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 96ms/step - loss: 0.6510 - mse: 0.2586 - mace: 20.8316 - val_loss: 0.7444 - val_mse: 0.3228 - val_mace: 23.8193

Epoch 00171: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 171/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6513 - mse: 0.2588 - mace: 20.8413  
Epoch 00171: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 97ms/step - loss: 0.6513 - mse: 0.2588 - mace: 20.8402 - val_loss: 0.7453 - val_mse: 0.3238 - val_mace: 23.8511

Epoch 00172: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 172/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6513 - mse: 0.2588 - mace: 20.8426  
Epoch 00172: val_loss did not improve from 0.73441
1016/1016 [==============================] - 108s 106ms/step - loss: 0.6513 - mse: 0.2588 - mace: 20.8432 - val_loss: 0.7436 - val_mse: 0.3217 - val_mace: 23.7951

Epoch 00173: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 173/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6517 - mse: 0.2591 - mace: 20.8530  
Epoch 00173: val_loss did not improve from 0.73441
1016/1016 [==============================] - 89s 88ms/step - loss: 0.6517 - mse: 0.2591 - mace: 20.8534 - val_loss: 0.7421 - val_mse: 0.3214 - val_mace: 23.7476

Epoch 00174: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 174/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6510 - mse: 0.2588 - mace: 20.8327  
Epoch 00174: val_loss did not improve from 0.73441
1016/1016 [==============================] - 97s 96ms/step - loss: 0.6510 - mse: 0.2588 - mace: 20.8318 - val_loss: 0.7423 - val_mse: 0.3216 - val_mace: 23.7532

Epoch 00175: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 175/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6530 - mse: 0.2599 - mace: 20.8954  
Epoch 00175: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 96ms/step - loss: 0.6530 - mse: 0.2599 - mace: 20.8955 - val_loss: 0.7420 - val_mse: 0.3208 - val_mace: 23.7448

Epoch 00176: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 176/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6514 - mse: 0.2588 - mace: 20.8461  
Epoch 00176: val_loss did not improve from 0.73441
1016/1016 [==============================] - 107s 106ms/step - loss: 0.6514 - mse: 0.2588 - mace: 20.8460 - val_loss: 0.7448 - val_mse: 0.3234 - val_mace: 23.8326

Epoch 00177: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 177/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6516 - mse: 0.2592 - mace: 20.8527  
Epoch 00177: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6517 - mse: 0.2592 - mace: 20.8540 - val_loss: 0.7442 - val_mse: 0.3225 - val_mace: 23.8155

Epoch 00178: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 178/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6490 - mse: 0.2574 - mace: 20.7665  
Epoch 00178: val_loss did not improve from 0.73441
1016/1016 [==============================] - 95s 93ms/step - loss: 0.6489 - mse: 0.2574 - mace: 20.7660 - val_loss: 0.7415 - val_mse: 0.3202 - val_mace: 23.7285

Epoch 00179: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 179/196
1014/1016 [============================>.] - ETA: 0s - loss: 0.6522 - mse: 0.2593 - mace: 20.8700  
Epoch 00179: val_loss did not improve from 0.73441
1016/1016 [==============================] - 93s 91ms/step - loss: 0.6522 - mse: 0.2594 - mace: 20.8700 - val_loss: 0.7407 - val_mse: 0.3195 - val_mace: 23.7029

Epoch 00180: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 180/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6507 - mse: 0.2585 - mace: 20.8225  
Epoch 00180: val_loss did not improve from 0.73441
1016/1016 [==============================] - 108s 107ms/step - loss: 0.6507 - mse: 0.2585 - mace: 20.8220 - val_loss: 0.7412 - val_mse: 0.3201 - val_mace: 23.7196

Epoch 00181: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 181/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6520 - mse: 0.2596 - mace: 20.8652 
Epoch 00181: val_loss did not improve from 0.73441
1016/1016 [==============================] - 94s 93ms/step - loss: 0.6520 - mse: 0.2596 - mace: 20.8646 - val_loss: 0.7431 - val_mse: 0.3215 - val_mace: 23.7801

Epoch 00182: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 182/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6506 - mse: 0.2584 - mace: 20.8196  
Epoch 00182: val_loss did not improve from 0.73441
1016/1016 [==============================] - 111s 109ms/step - loss: 0.6506 - mse: 0.2584 - mace: 20.8194 - val_loss: 0.7464 - val_mse: 0.3241 - val_mace: 23.8863

Epoch 00183: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 183/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6504 - mse: 0.2583 - mace: 20.8136  
Epoch 00183: val_loss did not improve from 0.73441
1016/1016 [==============================] - 94s 93ms/step - loss: 0.6504 - mse: 0.2583 - mace: 20.8128 - val_loss: 0.7418 - val_mse: 0.3208 - val_mace: 23.7378

Epoch 00184: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 184/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6505 - mse: 0.2582 - mace: 20.8145  
Epoch 00184: val_loss did not improve from 0.73441
1016/1016 [==============================] - 99s 98ms/step - loss: 0.6505 - mse: 0.2582 - mace: 20.8153 - val_loss: 0.7428 - val_mse: 0.3210 - val_mace: 23.7687

Epoch 00185: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 185/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6508 - mse: 0.2588 - mace: 20.8242  
Epoch 00185: val_loss did not improve from 0.73441
1016/1016 [==============================] - 96s 94ms/step - loss: 0.6508 - mse: 0.2588 - mace: 20.8240 - val_loss: 0.7427 - val_mse: 0.3211 - val_mace: 23.7672

Epoch 00186: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 186/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6515 - mse: 0.2590 - mace: 20.8467  
Epoch 00186: val_loss did not improve from 0.73441
1016/1016 [==============================] - 98s 97ms/step - loss: 0.6514 - mse: 0.2590 - mace: 20.8462 - val_loss: 0.7468 - val_mse: 0.3251 - val_mace: 23.8979

Epoch 00187: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 187/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6514 - mse: 0.2590 - mace: 20.8461  
Epoch 00187: val_loss did not improve from 0.73441
1016/1016 [==============================] - 101s 99ms/step - loss: 0.6514 - mse: 0.2589 - mace: 20.8459 - val_loss: 0.7411 - val_mse: 0.3198 - val_mace: 23.7144

Epoch 00188: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 188/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6526 - mse: 0.2600 - mace: 20.8824  
Epoch 00188: val_loss did not improve from 0.73441
1016/1016 [==============================] - 106s 104ms/step - loss: 0.6526 - mse: 0.2600 - mace: 20.8836 - val_loss: 0.7419 - val_mse: 0.3205 - val_mace: 23.7405

Epoch 00189: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 189/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6522 - mse: 0.2594 - mace: 20.8701 
Epoch 00189: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 98ms/step - loss: 0.6522 - mse: 0.2593 - mace: 20.8689 - val_loss: 0.7438 - val_mse: 0.3227 - val_mace: 23.8028

Epoch 00190: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 190/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6503 - mse: 0.2582 - mace: 20.8090  
Epoch 00190: val_loss did not improve from 0.73441
1016/1016 [==============================] - 102s 101ms/step - loss: 0.6503 - mse: 0.2582 - mace: 20.8096 - val_loss: 0.7407 - val_mse: 0.3194 - val_mace: 23.7033

Epoch 00191: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 191/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6514 - mse: 0.2589 - mace: 20.8458  
Epoch 00191: val_loss did not improve from 0.73441
1016/1016 [==============================] - 94s 92ms/step - loss: 0.6514 - mse: 0.2588 - mace: 20.8440 - val_loss: 0.7433 - val_mse: 0.3218 - val_mace: 23.7863

Epoch 00192: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 192/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6518 - mse: 0.2595 - mace: 20.8587  
Epoch 00192: val_loss did not improve from 0.73441
1016/1016 [==============================] - 94s 93ms/step - loss: 0.6518 - mse: 0.2595 - mace: 20.8581 - val_loss: 0.7418 - val_mse: 0.3209 - val_mace: 23.7379

Epoch 00193: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 193/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6488 - mse: 0.2573 - mace: 20.7629  
Epoch 00193: val_loss did not improve from 0.73441
1016/1016 [==============================] - 128s 126ms/step - loss: 0.6489 - mse: 0.2573 - mace: 20.7633 - val_loss: 0.7465 - val_mse: 0.3248 - val_mace: 23.8883

Epoch 00194: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 194/196
1014/1016 [============================>.] - ETA: 0s - loss: 0.6523 - mse: 0.2597 - mace: 20.8738  
Epoch 00194: val_loss did not improve from 0.73441
1016/1016 [==============================] - 102s 100ms/step - loss: 0.6524 - mse: 0.2597 - mace: 20.8762 - val_loss: 0.7428 - val_mse: 0.3213 - val_mace: 23.7709

Epoch 00195: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 195/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6499 - mse: 0.2580 - mace: 20.7983 
Epoch 00195: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 99ms/step - loss: 0.6499 - mse: 0.2580 - mace: 20.7979 - val_loss: 0.7406 - val_mse: 0.3194 - val_mace: 23.6984

Epoch 00196: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 196/196
1015/1016 [============================>.] - ETA: 0s - loss: 0.6516 - mse: 0.2590 - mace: 20.8508  
Epoch 00196: val_loss did not improve from 0.73441
1016/1016 [==============================] - 100s 99ms/step - loss: 0.6516 - mse: 0.2590 - mace: 20.8505 - val_loss: 0.7440 - val_mse: 0.3225 - val_mace: 23.8065

real	325m44.240s
user	289m59.275s
sys	13m42.983s
