time python train_transfer_learning.py
2019-03-16 07:02:00.245635: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-16 07:02:00.251043: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-03-16 07:02:00.358739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1009] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-16 07:02:00.359773: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55a2240 executing computations on platform CUDA. Devices:
2019-03-16 07:02:00.359791: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce RTX 2060, Compute Capability 7.5
2019-03-16 07:02:00.379918: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz
2019-03-16 07:02:00.380182: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x560ebc0 executing computations on platform Host. Devices:
2019-03-16 07:02:00.380203: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-16 07:02:00.380498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: 
name: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.83
pciBusID: 0000:01:00.0
totalMemory: 5.76GiB freeMemory: 5.31GiB
2019-03-16 07:02:00.380518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-03-16 07:02:00.380556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-03-16 07:02:00.381276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-16 07:02:00.381289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 
2019-03-16 07:02:00.381295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N 
2019-03-16 07:02:00.381405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5134 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-03-16 07:02:00.391021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-03-16 07:02:00.391059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-16 07:02:00.391068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 
2019-03-16 07:02:00.391074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N 
2019-03-16 07:02:00.391182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5134 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)
40670
8134
epochs: 98
epochs per stage: 24

Epoch 00001: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 1/98
2019-03-16 07:02:18.976502: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-03-16 07:02:19.177629: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-03-16 07:02:20.261896: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profile Session started.
2019-03-16 07:02:20.262772: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcupti.so.10.0
   1/2033 [..............................] - ETA: 7:37:18 - loss: 2.8962 - mse: 5.2577 - mace: 92.6777WARNING: Logging before flag parsing goes to stderr.
W0316 07:02:20.625118 140525664917248 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.157991). Check your callbacks.
2032/2033 [============================>.] - ETA: 0s - loss: 0.7843 - mse: 0.3705 - mace: 25.0991     
Epoch 00001: val_loss improved from inf to 0.76666, saving model to checkpoint.h5
2033/2033 [==============================] - 142s 70ms/step - loss: 0.7843 - mse: 0.3704 - mace: 25.0978 - val_loss: 0.7667 - val_mse: 0.3347 - val_mace: 24.5330

Epoch 00002: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 2/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7658 - mse: 0.3338 - mace: 24.5041  
Epoch 00002: val_loss improved from 0.76666 to 0.76282, saving model to checkpoint.h5
2033/2033 [==============================] - 129s 64ms/step - loss: 0.7658 - mse: 0.3338 - mace: 24.5044 - val_loss: 0.7628 - val_mse: 0.3315 - val_mace: 24.4101

Epoch 00003: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 3/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7650 - mse: 0.3334 - mace: 24.4794  
Epoch 00003: val_loss improved from 0.76282 to 0.76219, saving model to checkpoint.h5
2033/2033 [==============================] - 128s 63ms/step - loss: 0.7650 - mse: 0.3334 - mace: 24.4794 - val_loss: 0.7622 - val_mse: 0.3311 - val_mace: 24.3901

Epoch 00004: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 4/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7651 - mse: 0.3336 - mace: 24.4827  
Epoch 00004: val_loss did not improve from 0.76219
2033/2033 [==============================] - 135s 67ms/step - loss: 0.7651 - mse: 0.3335 - mace: 24.4819 - val_loss: 0.7631 - val_mse: 0.3317 - val_mace: 24.4189

Epoch 00005: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 5/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7655 - mse: 0.3339 - mace: 24.4959  
Epoch 00005: val_loss did not improve from 0.76219
2033/2033 [==============================] - 126s 62ms/step - loss: 0.7655 - mse: 0.3339 - mace: 24.4967 - val_loss: 0.7661 - val_mse: 0.3337 - val_mace: 24.5151

Epoch 00006: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 6/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7625 - mse: 0.3322 - mace: 24.4010  
Epoch 00006: val_loss improved from 0.76219 to 0.75871, saving model to checkpoint.h5
2033/2033 [==============================] - 122s 60ms/step - loss: 0.7625 - mse: 0.3322 - mace: 24.4008 - val_loss: 0.7587 - val_mse: 0.3286 - val_mace: 24.2786

Epoch 00007: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 7/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7557 - mse: 0.3274 - mace: 24.1811  
Epoch 00007: val_loss improved from 0.75871 to 0.75321, saving model to checkpoint.h5
2033/2033 [==============================] - 134s 66ms/step - loss: 0.7556 - mse: 0.3273 - mace: 24.1799 - val_loss: 0.7532 - val_mse: 0.3247 - val_mace: 24.1026

Epoch 00008: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 8/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7566 - mse: 0.3277 - mace: 24.2098  
Epoch 00008: val_loss improved from 0.75321 to 0.74938, saving model to checkpoint.h5
2033/2033 [==============================] - 136s 67ms/step - loss: 0.7566 - mse: 0.3277 - mace: 24.2108 - val_loss: 0.7494 - val_mse: 0.3220 - val_mace: 23.9803

Epoch 00009: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 9/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7545 - mse: 0.3275 - mace: 24.1448  
Epoch 00009: val_loss did not improve from 0.74938
2033/2033 [==============================] - 120s 59ms/step - loss: 0.7545 - mse: 0.3275 - mace: 24.1442 - val_loss: 0.7510 - val_mse: 0.3222 - val_mace: 24.0319

Epoch 00010: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 10/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7477 - mse: 0.3225 - mace: 23.9250  
Epoch 00010: val_loss did not improve from 0.74938
2033/2033 [==============================] - 122s 60ms/step - loss: 0.7477 - mse: 0.3225 - mace: 23.9256 - val_loss: 0.7501 - val_mse: 0.3223 - val_mace: 24.0039

Epoch 00011: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 11/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7460 - mse: 0.3216 - mace: 23.8705  
Epoch 00011: val_loss improved from 0.74938 to 0.74646, saving model to checkpoint.h5
2033/2033 [==============================] - 117s 58ms/step - loss: 0.7460 - mse: 0.3216 - mace: 23.8713 - val_loss: 0.7465 - val_mse: 0.3201 - val_mace: 23.8868

Epoch 00012: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 12/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7436 - mse: 0.3206 - mace: 23.7948  
Epoch 00012: val_loss improved from 0.74646 to 0.73847, saving model to checkpoint.h5
2033/2033 [==============================] - 124s 61ms/step - loss: 0.7436 - mse: 0.3206 - mace: 23.7953 - val_loss: 0.7385 - val_mse: 0.3128 - val_mace: 23.6309

Epoch 00013: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 13/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7366 - mse: 0.3158 - mace: 23.5709  
Epoch 00013: val_loss improved from 0.73847 to 0.73845, saving model to checkpoint.h5
2033/2033 [==============================] - 129s 63ms/step - loss: 0.7366 - mse: 0.3158 - mace: 23.5712 - val_loss: 0.7385 - val_mse: 0.3126 - val_mace: 23.6305

Epoch 00014: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 14/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7322 - mse: 0.3129 - mace: 23.4312  
Epoch 00014: val_loss improved from 0.73845 to 0.73730, saving model to checkpoint.h5
2033/2033 [==============================] - 136s 67ms/step - loss: 0.7322 - mse: 0.3129 - mace: 23.4314 - val_loss: 0.7373 - val_mse: 0.3118 - val_mace: 23.5935

Epoch 00015: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 15/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7268 - mse: 0.3090 - mace: 23.2578  
Epoch 00015: val_loss did not improve from 0.73730
2033/2033 [==============================] - 124s 61ms/step - loss: 0.7268 - mse: 0.3090 - mace: 23.2580 - val_loss: 0.7497 - val_mse: 0.3235 - val_mace: 23.9908

Epoch 00016: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 16/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7249 - mse: 0.3077 - mace: 23.1977  
Epoch 00016: val_loss did not improve from 0.73730
2033/2033 [==============================] - 121s 59ms/step - loss: 0.7249 - mse: 0.3078 - mace: 23.1978 - val_loss: 0.7383 - val_mse: 0.3142 - val_mace: 23.6266

Epoch 00017: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 17/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7244 - mse: 0.3071 - mace: 23.1797   
Epoch 00017: val_loss did not improve from 0.73730
2033/2033 [==============================] - 125s 62ms/step - loss: 0.7244 - mse: 0.3071 - mace: 23.1801 - val_loss: 0.7444 - val_mse: 0.3221 - val_mace: 23.8224

Epoch 00018: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 18/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7240 - mse: 0.3071 - mace: 23.1678  
Epoch 00018: val_loss did not improve from 0.73730
2033/2033 [==============================] - 124s 61ms/step - loss: 0.7240 - mse: 0.3071 - mace: 23.1683 - val_loss: 0.7417 - val_mse: 0.3164 - val_mace: 23.7352

Epoch 00019: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 19/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7221 - mse: 0.3062 - mace: 23.1058  
Epoch 00019: val_loss did not improve from 0.73730
2033/2033 [==============================] - 126s 62ms/step - loss: 0.7220 - mse: 0.3062 - mace: 23.1053 - val_loss: 0.7478 - val_mse: 0.3221 - val_mace: 23.9288

Epoch 00020: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 20/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7237 - mse: 0.3080 - mace: 23.1576  
Epoch 00020: val_loss improved from 0.73730 to 0.73709, saving model to checkpoint.h5
2033/2033 [==============================] - 127s 62ms/step - loss: 0.7237 - mse: 0.3080 - mace: 23.1589 - val_loss: 0.7371 - val_mse: 0.3126 - val_mace: 23.5868

Epoch 00021: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 21/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7188 - mse: 0.3039 - mace: 23.0022  
Epoch 00021: val_loss did not improve from 0.73709
2033/2033 [==============================] - 121s 59ms/step - loss: 0.7188 - mse: 0.3039 - mace: 23.0019 - val_loss: 0.7474 - val_mse: 0.3219 - val_mace: 23.9160

Epoch 00022: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 22/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7176 - mse: 0.3032 - mace: 22.9631  
Epoch 00022: val_loss did not improve from 0.73709
2033/2033 [==============================] - 112s 55ms/step - loss: 0.7176 - mse: 0.3032 - mace: 22.9623 - val_loss: 0.7403 - val_mse: 0.3173 - val_mace: 23.6906

Epoch 00023: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 23/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7171 - mse: 0.3028 - mace: 22.9466  
Epoch 00023: val_loss did not improve from 0.73709
2033/2033 [==============================] - 127s 62ms/step - loss: 0.7171 - mse: 0.3029 - mace: 22.9471 - val_loss: 0.7424 - val_mse: 0.3180 - val_mace: 23.7555

Epoch 00024: LearningRateScheduler reducing learning rate to 0.004999999888241291.
Epoch 24/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7161 - mse: 0.3023 - mace: 22.9137  
Epoch 00024: val_loss did not improve from 0.73709
2033/2033 [==============================] - 126s 62ms/step - loss: 0.7160 - mse: 0.3023 - mace: 22.9136 - val_loss: 0.7490 - val_mse: 0.3234 - val_mace: 23.9696

Epoch 00025: LearningRateScheduler reducing learning rate to 0.0004999999888241291.
Epoch 25/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7114 - mse: 0.2983 - mace: 22.7662  
Epoch 00025: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 61ms/step - loss: 0.7115 - mse: 0.2983 - mace: 22.7668 - val_loss: 0.7492 - val_mse: 0.3251 - val_mace: 23.9751

Epoch 00026: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 26/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7098 - mse: 0.2973 - mace: 22.7124  
Epoch 00026: val_loss did not improve from 0.73709
2033/2033 [==============================] - 124s 61ms/step - loss: 0.7097 - mse: 0.2972 - mace: 22.7109 - val_loss: 0.7476 - val_mse: 0.3225 - val_mace: 23.9230

Epoch 00027: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 27/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7077 - mse: 0.2958 - mace: 22.6458   
Epoch 00027: val_loss did not improve from 0.73709
2033/2033 [==============================] - 127s 63ms/step - loss: 0.7077 - mse: 0.2958 - mace: 22.6449 - val_loss: 0.7472 - val_mse: 0.3231 - val_mace: 23.9106

Epoch 00028: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 28/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7072 - mse: 0.2956 - mace: 22.6316  
Epoch 00028: val_loss did not improve from 0.73709
2033/2033 [==============================] - 116s 57ms/step - loss: 0.7072 - mse: 0.2956 - mace: 22.6315 - val_loss: 0.7428 - val_mse: 0.3189 - val_mace: 23.7682

Epoch 00029: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 29/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7060 - mse: 0.2951 - mace: 22.5925  
Epoch 00029: val_loss did not improve from 0.73709
2033/2033 [==============================] - 124s 61ms/step - loss: 0.7060 - mse: 0.2951 - mace: 22.5925 - val_loss: 0.7474 - val_mse: 0.3237 - val_mace: 23.9182

Epoch 00030: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 30/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7060 - mse: 0.2952 - mace: 22.5907  
Epoch 00030: val_loss did not improve from 0.73709
2033/2033 [==============================] - 124s 61ms/step - loss: 0.7060 - mse: 0.2952 - mace: 22.5914 - val_loss: 0.7422 - val_mse: 0.3188 - val_mace: 23.7493

Epoch 00031: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 31/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7030 - mse: 0.2930 - mace: 22.4952   
Epoch 00031: val_loss did not improve from 0.73709
2033/2033 [==============================] - 125s 61ms/step - loss: 0.7029 - mse: 0.2930 - mace: 22.4942 - val_loss: 0.7510 - val_mse: 0.3278 - val_mace: 24.0330

Epoch 00032: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 32/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7046 - mse: 0.2942 - mace: 22.5488  
Epoch 00032: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 60ms/step - loss: 0.7046 - mse: 0.2941 - mace: 22.5480 - val_loss: 0.7447 - val_mse: 0.3215 - val_mace: 23.8296

Epoch 00033: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 33/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7039 - mse: 0.2937 - mace: 22.5250  
Epoch 00033: val_loss did not improve from 0.73709
2033/2033 [==============================] - 122s 60ms/step - loss: 0.7039 - mse: 0.2937 - mace: 22.5257 - val_loss: 0.7485 - val_mse: 0.3245 - val_mace: 23.9506

Epoch 00034: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 34/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7028 - mse: 0.2928 - mace: 22.4893  
Epoch 00034: val_loss did not improve from 0.73709
2033/2033 [==============================] - 122s 60ms/step - loss: 0.7028 - mse: 0.2928 - mace: 22.4891 - val_loss: 0.7446 - val_mse: 0.3217 - val_mace: 23.8285

Epoch 00035: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 35/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7034 - mse: 0.2932 - mace: 22.5085  
Epoch 00035: val_loss did not improve from 0.73709
2033/2033 [==============================] - 121s 60ms/step - loss: 0.7034 - mse: 0.2932 - mace: 22.5078 - val_loss: 0.7470 - val_mse: 0.3231 - val_mace: 23.9040

Epoch 00036: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 36/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7038 - mse: 0.2937 - mace: 22.5221  
Epoch 00036: val_loss did not improve from 0.73709
2033/2033 [==============================] - 126s 62ms/step - loss: 0.7038 - mse: 0.2937 - mace: 22.5214 - val_loss: 0.7459 - val_mse: 0.3213 - val_mace: 23.8679

Epoch 00037: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 37/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7019 - mse: 0.2924 - mace: 22.4614   
Epoch 00037: val_loss did not improve from 0.73709
2033/2033 [==============================] - 133s 65ms/step - loss: 0.7019 - mse: 0.2924 - mace: 22.4601 - val_loss: 0.7429 - val_mse: 0.3192 - val_mace: 23.7733

Epoch 00038: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 38/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7015 - mse: 0.2919 - mace: 22.4482  
Epoch 00038: val_loss did not improve from 0.73709
2033/2033 [==============================] - 117s 58ms/step - loss: 0.7015 - mse: 0.2919 - mace: 22.4485 - val_loss: 0.7444 - val_mse: 0.3212 - val_mace: 23.8193

Epoch 00039: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 39/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7012 - mse: 0.2917 - mace: 22.4370  
Epoch 00039: val_loss did not improve from 0.73709
2033/2033 [==============================] - 118s 58ms/step - loss: 0.7012 - mse: 0.2918 - mace: 22.4369 - val_loss: 0.7436 - val_mse: 0.3208 - val_mace: 23.7956

Epoch 00040: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 40/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7007 - mse: 0.2912 - mace: 22.4226  
Epoch 00040: val_loss did not improve from 0.73709
2033/2033 [==============================] - 121s 60ms/step - loss: 0.7007 - mse: 0.2912 - mace: 22.4227 - val_loss: 0.7402 - val_mse: 0.3173 - val_mace: 23.6849

Epoch 00041: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 41/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6997 - mse: 0.2907 - mace: 22.3892   
Epoch 00041: val_loss did not improve from 0.73709
2033/2033 [==============================] - 127s 63ms/step - loss: 0.6997 - mse: 0.2907 - mace: 22.3897 - val_loss: 0.7450 - val_mse: 0.3218 - val_mace: 23.8407

Epoch 00042: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 42/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6984 - mse: 0.2899 - mace: 22.3484  
Epoch 00042: val_loss did not improve from 0.73709
2033/2033 [==============================] - 121s 60ms/step - loss: 0.6984 - mse: 0.2899 - mace: 22.3477 - val_loss: 0.7460 - val_mse: 0.3224 - val_mace: 23.8730

Epoch 00043: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 43/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6979 - mse: 0.2894 - mace: 22.3332  
Epoch 00043: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 61ms/step - loss: 0.6979 - mse: 0.2894 - mace: 22.3329 - val_loss: 0.7420 - val_mse: 0.3196 - val_mace: 23.7437

Epoch 00044: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 44/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.7006 - mse: 0.2912 - mace: 22.4199  
Epoch 00044: val_loss did not improve from 0.73709
2033/2033 [==============================] - 117s 58ms/step - loss: 0.7006 - mse: 0.2912 - mace: 22.4197 - val_loss: 0.7436 - val_mse: 0.3195 - val_mace: 23.7949

Epoch 00045: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 45/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.7000 - mse: 0.2911 - mace: 22.3990   
Epoch 00045: val_loss did not improve from 0.73709
2033/2033 [==============================] - 134s 66ms/step - loss: 0.7000 - mse: 0.2911 - mace: 22.4006 - val_loss: 0.7428 - val_mse: 0.3189 - val_mace: 23.7710

Epoch 00046: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 46/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6984 - mse: 0.2900 - mace: 22.3503  
Epoch 00046: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 61ms/step - loss: 0.6985 - mse: 0.2900 - mace: 22.3510 - val_loss: 0.7487 - val_mse: 0.3257 - val_mace: 23.9589

Epoch 00047: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 47/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6980 - mse: 0.2899 - mace: 22.3365  
Epoch 00047: val_loss did not improve from 0.73709
2033/2033 [==============================] - 130s 64ms/step - loss: 0.6980 - mse: 0.2899 - mace: 22.3360 - val_loss: 0.7436 - val_mse: 0.3189 - val_mace: 23.7956

Epoch 00048: LearningRateScheduler reducing learning rate to 0.0004999999655410647.
Epoch 48/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6983 - mse: 0.2897 - mace: 22.3464  
Epoch 00048: val_loss did not improve from 0.73709
2033/2033 [==============================] - 124s 61ms/step - loss: 0.6983 - mse: 0.2897 - mace: 22.3467 - val_loss: 0.7425 - val_mse: 0.3200 - val_mace: 23.7609

Epoch 00049: LearningRateScheduler reducing learning rate to 4.9999996554106475e-05.
Epoch 49/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6977 - mse: 0.2893 - mace: 22.3276  
Epoch 00049: val_loss did not improve from 0.73709
2033/2033 [==============================] - 117s 57ms/step - loss: 0.6977 - mse: 0.2893 - mace: 22.3279 - val_loss: 0.7432 - val_mse: 0.3206 - val_mace: 23.7823

Epoch 00050: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 50/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6969 - mse: 0.2887 - mace: 22.2999  
Epoch 00050: val_loss did not improve from 0.73709
2033/2033 [==============================] - 112s 55ms/step - loss: 0.6969 - mse: 0.2887 - mace: 22.2998 - val_loss: 0.7395 - val_mse: 0.3173 - val_mace: 23.6654

Epoch 00051: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 51/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6975 - mse: 0.2891 - mace: 22.3202   
Epoch 00051: val_loss did not improve from 0.73709
2033/2033 [==============================] - 131s 64ms/step - loss: 0.6975 - mse: 0.2892 - mace: 22.3215 - val_loss: 0.7416 - val_mse: 0.3191 - val_mace: 23.7322

Epoch 00052: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 52/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6951 - mse: 0.2874 - mace: 22.2437  
Epoch 00052: val_loss did not improve from 0.73709
2033/2033 [==============================] - 122s 60ms/step - loss: 0.6951 - mse: 0.2874 - mace: 22.2434 - val_loss: 0.7459 - val_mse: 0.3222 - val_mace: 23.8677

Epoch 00053: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 53/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6948 - mse: 0.2873 - mace: 22.2321  
Epoch 00053: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 60ms/step - loss: 0.6947 - mse: 0.2873 - mace: 22.2312 - val_loss: 0.7414 - val_mse: 0.3185 - val_mace: 23.7233

Epoch 00054: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 54/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6959 - mse: 0.2885 - mace: 22.2702  
Epoch 00054: val_loss did not improve from 0.73709
2033/2033 [==============================] - 127s 62ms/step - loss: 0.6959 - mse: 0.2885 - mace: 22.2699 - val_loss: 0.7419 - val_mse: 0.3184 - val_mace: 23.7394

Epoch 00055: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 55/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6957 - mse: 0.2879 - mace: 22.2618  
Epoch 00055: val_loss did not improve from 0.73709
2033/2033 [==============================] - 116s 57ms/step - loss: 0.6957 - mse: 0.2879 - mace: 22.2615 - val_loss: 0.7451 - val_mse: 0.3213 - val_mace: 23.8429

Epoch 00056: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 56/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6954 - mse: 0.2876 - mace: 22.2534  
Epoch 00056: val_loss did not improve from 0.73709
2033/2033 [==============================] - 125s 61ms/step - loss: 0.6954 - mse: 0.2876 - mace: 22.2543 - val_loss: 0.7466 - val_mse: 0.3231 - val_mace: 23.8901

Epoch 00057: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 57/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6989 - mse: 0.2904 - mace: 22.3659  
Epoch 00057: val_loss did not improve from 0.73709
2033/2033 [==============================] - 135s 66ms/step - loss: 0.6990 - mse: 0.2904 - mace: 22.3673 - val_loss: 0.7432 - val_mse: 0.3195 - val_mace: 23.7840

Epoch 00058: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 58/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6943 - mse: 0.2872 - mace: 22.2177  
Epoch 00058: val_loss did not improve from 0.73709
2033/2033 [==============================] - 127s 62ms/step - loss: 0.6943 - mse: 0.2871 - mace: 22.2170 - val_loss: 0.7437 - val_mse: 0.3205 - val_mace: 23.7995

Epoch 00059: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 59/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6964 - mse: 0.2886 - mace: 22.2836  
Epoch 00059: val_loss did not improve from 0.73709
2033/2033 [==============================] - 124s 61ms/step - loss: 0.6963 - mse: 0.2886 - mace: 22.2830 - val_loss: 0.7408 - val_mse: 0.3179 - val_mace: 23.7052

Epoch 00060: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 60/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6939 - mse: 0.2867 - mace: 22.2037  
Epoch 00060: val_loss did not improve from 0.73709
2033/2033 [==============================] - 117s 58ms/step - loss: 0.6939 - mse: 0.2867 - mace: 22.2039 - val_loss: 0.7396 - val_mse: 0.3167 - val_mace: 23.6665

Epoch 00061: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 61/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6951 - mse: 0.2876 - mace: 22.2447  
Epoch 00061: val_loss did not improve from 0.73709
2033/2033 [==============================] - 127s 63ms/step - loss: 0.6951 - mse: 0.2876 - mace: 22.2441 - val_loss: 0.7450 - val_mse: 0.3210 - val_mace: 23.8410

Epoch 00062: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 62/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6935 - mse: 0.2863 - mace: 22.1906  
Epoch 00062: val_loss did not improve from 0.73709
2033/2033 [==============================] - 128s 63ms/step - loss: 0.6934 - mse: 0.2863 - mace: 22.1898 - val_loss: 0.7419 - val_mse: 0.3187 - val_mace: 23.7403

Epoch 00063: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 63/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6952 - mse: 0.2877 - mace: 22.2450  
Epoch 00063: val_loss did not improve from 0.73709
2033/2033 [==============================] - 120s 59ms/step - loss: 0.6952 - mse: 0.2877 - mace: 22.2461 - val_loss: 0.7449 - val_mse: 0.3214 - val_mace: 23.8373

Epoch 00064: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 64/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6957 - mse: 0.2883 - mace: 22.2638  
Epoch 00064: val_loss did not improve from 0.73709
2033/2033 [==============================] - 124s 61ms/step - loss: 0.6957 - mse: 0.2883 - mace: 22.2635 - val_loss: 0.7475 - val_mse: 0.3233 - val_mace: 23.9192

Epoch 00065: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 65/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6935 - mse: 0.2868 - mace: 22.1925   
Epoch 00065: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 60ms/step - loss: 0.6935 - mse: 0.2868 - mace: 22.1924 - val_loss: 0.7414 - val_mse: 0.3186 - val_mace: 23.7235

Epoch 00066: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 66/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6948 - mse: 0.2875 - mace: 22.2325  
Epoch 00066: val_loss did not improve from 0.73709
2033/2033 [==============================] - 115s 57ms/step - loss: 0.6948 - mse: 0.2875 - mace: 22.2325 - val_loss: 0.7422 - val_mse: 0.3190 - val_mace: 23.7496

Epoch 00067: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 67/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6949 - mse: 0.2876 - mace: 22.2372  
Epoch 00067: val_loss did not improve from 0.73709
2033/2033 [==============================] - 125s 62ms/step - loss: 0.6949 - mse: 0.2876 - mace: 22.2367 - val_loss: 0.7417 - val_mse: 0.3192 - val_mace: 23.7352

Epoch 00068: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 68/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6949 - mse: 0.2876 - mace: 22.2373  
Epoch 00068: val_loss did not improve from 0.73709
2033/2033 [==============================] - 128s 63ms/step - loss: 0.6949 - mse: 0.2876 - mace: 22.2371 - val_loss: 0.7424 - val_mse: 0.3198 - val_mace: 23.7556

Epoch 00069: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 69/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6937 - mse: 0.2864 - mace: 22.1983  
Epoch 00069: val_loss did not improve from 0.73709
2033/2033 [==============================] - 125s 62ms/step - loss: 0.6937 - mse: 0.2865 - mace: 22.1987 - val_loss: 0.7445 - val_mse: 0.3214 - val_mace: 23.8253

Epoch 00070: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 70/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6949 - mse: 0.2875 - mace: 22.2364  
Epoch 00070: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 60ms/step - loss: 0.6949 - mse: 0.2875 - mace: 22.2355 - val_loss: 0.7423 - val_mse: 0.3193 - val_mace: 23.7547

Epoch 00071: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 71/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6941 - mse: 0.2869 - mace: 22.2111  
Epoch 00071: val_loss did not improve from 0.73709
2033/2033 [==============================] - 119s 59ms/step - loss: 0.6941 - mse: 0.2870 - mace: 22.2125 - val_loss: 0.7475 - val_mse: 0.3240 - val_mace: 23.9200

Epoch 00072: LearningRateScheduler reducing learning rate to 4.999999509891495e-05.
Epoch 72/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6924 - mse: 0.2862 - mace: 22.1558  
Epoch 00072: val_loss did not improve from 0.73709
2033/2033 [==============================] - 121s 59ms/step - loss: 0.6924 - mse: 0.2861 - mace: 22.1556 - val_loss: 0.7450 - val_mse: 0.3209 - val_mace: 23.8396

Epoch 00073: LearningRateScheduler reducing learning rate to 4.999999509891496e-06.
Epoch 73/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6932 - mse: 0.2865 - mace: 22.1832  
Epoch 00073: val_loss did not improve from 0.73709
2033/2033 [==============================] - 133s 66ms/step - loss: 0.6932 - mse: 0.2865 - mace: 22.1832 - val_loss: 0.7413 - val_mse: 0.3186 - val_mace: 23.7209

Epoch 00074: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 74/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6942 - mse: 0.2873 - mace: 22.2146  
Epoch 00074: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 60ms/step - loss: 0.6942 - mse: 0.2873 - mace: 22.2147 - val_loss: 0.7416 - val_mse: 0.3188 - val_mace: 23.7314

Epoch 00075: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 75/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6945 - mse: 0.2872 - mace: 22.2249  
Epoch 00075: val_loss did not improve from 0.73709
2033/2033 [==============================] - 125s 61ms/step - loss: 0.6946 - mse: 0.2872 - mace: 22.2258 - val_loss: 0.7426 - val_mse: 0.3197 - val_mace: 23.7633

Epoch 00076: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 76/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6947 - mse: 0.2875 - mace: 22.2307  
Epoch 00076: val_loss did not improve from 0.73709
2033/2033 [==============================] - 129s 64ms/step - loss: 0.6947 - mse: 0.2875 - mace: 22.2306 - val_loss: 0.7427 - val_mse: 0.3191 - val_mace: 23.7664

Epoch 00077: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 77/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6955 - mse: 0.2880 - mace: 22.2569  
Epoch 00077: val_loss did not improve from 0.73709
2033/2033 [==============================] - 129s 63ms/step - loss: 0.6955 - mse: 0.2879 - mace: 22.2558 - val_loss: 0.7401 - val_mse: 0.3174 - val_mace: 23.6841

Epoch 00078: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 78/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6937 - mse: 0.2865 - mace: 22.1999  
Epoch 00078: val_loss did not improve from 0.73709
2033/2033 [==============================] - 125s 62ms/step - loss: 0.6938 - mse: 0.2865 - mace: 22.2000 - val_loss: 0.7421 - val_mse: 0.3193 - val_mace: 23.7480

Epoch 00079: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 79/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6930 - mse: 0.2864 - mace: 22.1776  
Epoch 00079: val_loss did not improve from 0.73709
2033/2033 [==============================] - 124s 61ms/step - loss: 0.6930 - mse: 0.2864 - mace: 22.1766 - val_loss: 0.7455 - val_mse: 0.3218 - val_mace: 23.8554

Epoch 00080: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 80/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6944 - mse: 0.2871 - mace: 22.2207  
Epoch 00080: val_loss did not improve from 0.73709
2033/2033 [==============================] - 125s 62ms/step - loss: 0.6944 - mse: 0.2871 - mace: 22.2209 - val_loss: 0.7449 - val_mse: 0.3213 - val_mace: 23.8382

Epoch 00081: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 81/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6929 - mse: 0.2863 - mace: 22.1720  
Epoch 00081: val_loss did not improve from 0.73709
2033/2033 [==============================] - 128s 63ms/step - loss: 0.6929 - mse: 0.2863 - mace: 22.1723 - val_loss: 0.7418 - val_mse: 0.3190 - val_mace: 23.7363

Epoch 00082: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 82/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6947 - mse: 0.2876 - mace: 22.2306  
Epoch 00082: val_loss did not improve from 0.73709
2033/2033 [==============================] - 115s 57ms/step - loss: 0.6947 - mse: 0.2876 - mace: 22.2316 - val_loss: 0.7423 - val_mse: 0.3191 - val_mace: 23.7533

Epoch 00083: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 83/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6943 - mse: 0.2871 - mace: 22.2167  
Epoch 00083: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 60ms/step - loss: 0.6943 - mse: 0.2871 - mace: 22.2162 - val_loss: 0.7422 - val_mse: 0.3196 - val_mace: 23.7498

Epoch 00084: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 84/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6932 - mse: 0.2861 - mace: 22.1817  
Epoch 00084: val_loss did not improve from 0.73709
2033/2033 [==============================] - 121s 60ms/step - loss: 0.6932 - mse: 0.2862 - mace: 22.1825 - val_loss: 0.7412 - val_mse: 0.3185 - val_mace: 23.7180

Epoch 00085: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 85/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6941 - mse: 0.2869 - mace: 22.2097  
Epoch 00085: val_loss did not improve from 0.73709
2033/2033 [==============================] - 129s 63ms/step - loss: 0.6940 - mse: 0.2869 - mace: 22.2094 - val_loss: 0.7420 - val_mse: 0.3190 - val_mace: 23.7438

Epoch 00086: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 86/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6933 - mse: 0.2864 - mace: 22.1842  
Epoch 00086: val_loss did not improve from 0.73709
2033/2033 [==============================] - 124s 61ms/step - loss: 0.6933 - mse: 0.2864 - mace: 22.1846 - val_loss: 0.7422 - val_mse: 0.3190 - val_mace: 23.7511

Epoch 00087: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 87/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6948 - mse: 0.2874 - mace: 22.2330  
Epoch 00087: val_loss did not improve from 0.73709
2033/2033 [==============================] - 116s 57ms/step - loss: 0.6948 - mse: 0.2873 - mace: 22.2326 - val_loss: 0.7425 - val_mse: 0.3193 - val_mace: 23.7608

Epoch 00088: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 88/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6927 - mse: 0.2861 - mace: 22.1654  
Epoch 00088: val_loss did not improve from 0.73709
2033/2033 [==============================] - 125s 61ms/step - loss: 0.6926 - mse: 0.2860 - mace: 22.1640 - val_loss: 0.7448 - val_mse: 0.3209 - val_mace: 23.8345

Epoch 00089: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 89/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6938 - mse: 0.2868 - mace: 22.2008   
Epoch 00089: val_loss did not improve from 0.73709
2033/2033 [==============================] - 126s 62ms/step - loss: 0.6938 - mse: 0.2868 - mace: 22.2001 - val_loss: 0.7417 - val_mse: 0.3188 - val_mace: 23.7344

Epoch 00090: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 90/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6936 - mse: 0.2866 - mace: 22.1946  
Epoch 00090: val_loss did not improve from 0.73709
2033/2033 [==============================] - 124s 61ms/step - loss: 0.6936 - mse: 0.2866 - mace: 22.1949 - val_loss: 0.7457 - val_mse: 0.3221 - val_mace: 23.8618

Epoch 00091: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 91/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6926 - mse: 0.2860 - mace: 22.1634  
Epoch 00091: val_loss did not improve from 0.73709
2033/2033 [==============================] - 131s 64ms/step - loss: 0.6926 - mse: 0.2860 - mace: 22.1638 - val_loss: 0.7452 - val_mse: 0.3218 - val_mace: 23.8450

Epoch 00092: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 92/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6942 - mse: 0.2871 - mace: 22.2131  
Epoch 00092: val_loss did not improve from 0.73709
2033/2033 [==============================] - 120s 59ms/step - loss: 0.6942 - mse: 0.2872 - mace: 22.2140 - val_loss: 0.7429 - val_mse: 0.3194 - val_mace: 23.7718

Epoch 00093: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 93/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6949 - mse: 0.2876 - mace: 22.2363  
Epoch 00093: val_loss did not improve from 0.73709
2033/2033 [==============================] - 114s 56ms/step - loss: 0.6949 - mse: 0.2876 - mace: 22.2371 - val_loss: 0.7426 - val_mse: 0.3193 - val_mace: 23.7618

Epoch 00094: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 94/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6930 - mse: 0.2863 - mace: 22.1767  
Epoch 00094: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 60ms/step - loss: 0.6931 - mse: 0.2864 - mace: 22.1785 - val_loss: 0.7418 - val_mse: 0.3195 - val_mace: 23.7376

Epoch 00095: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 95/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6947 - mse: 0.2873 - mace: 22.2299  
Epoch 00095: val_loss did not improve from 0.73709
2033/2033 [==============================] - 129s 64ms/step - loss: 0.6947 - mse: 0.2873 - mace: 22.2297 - val_loss: 0.7458 - val_mse: 0.3219 - val_mace: 23.8644

Epoch 00096: LearningRateScheduler reducing learning rate to 4.999999418942025e-06.
Epoch 96/98
2032/2033 [============================>.] - ETA: 0s - loss: 0.6922 - mse: 0.2856 - mace: 22.1489  
Epoch 00096: val_loss did not improve from 0.73709
2033/2033 [==============================] - 123s 60ms/step - loss: 0.6921 - mse: 0.2856 - mace: 22.1486 - val_loss: 0.7431 - val_mse: 0.3198 - val_mace: 23.7783

Epoch 00097: LearningRateScheduler reducing learning rate to 4.999999418942025e-07.
Epoch 97/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6949 - mse: 0.2876 - mace: 22.2380  
Epoch 00097: val_loss did not improve from 0.73709
2033/2033 [==============================] - 128s 63ms/step - loss: 0.6949 - mse: 0.2876 - mace: 22.2371 - val_loss: 0.7411 - val_mse: 0.3181 - val_mace: 23.7148

Epoch 00098: LearningRateScheduler reducing learning rate to 4.999999418942025e-07.
Epoch 98/98
2031/2033 [============================>.] - ETA: 0s - loss: 0.6944 - mse: 0.2872 - mace: 22.2222  
Epoch 00098: val_loss did not improve from 0.73709
2033/2033 [==============================] - 115s 57ms/step - loss: 0.6944 - mse: 0.2871 - mace: 22.2218 - val_loss: 0.7402 - val_mse: 0.3175 - val_mace: 23.6858

real	203m11.726s
user	193m25.334s
sys	9m33.695s